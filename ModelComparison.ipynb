{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModelComparison.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOi9XYnXCQdMs18xzOIzwmC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sashutosh/DeepLearning/blob/master/ModelComparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUATgKWxkeSU"
      },
      "source": [
        "#RAS Busyness Prediction model\r\n",
        "This is a model comparison exercise to compare across various models and determine which model fits best and also which parameter most affect the busyness of the worker\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipbzGKlSiKoj"
      },
      "source": [
        "import pandas as pd\r\n",
        "import plotly.express as px\r\n",
        "import numpy as np\r\n",
        "import plotly.graph_objects as go\r\n",
        "from plotly.subplots import make_subplots\r\n",
        "from sklearn.model_selection import train_test_split    "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNjRIXLukyjs"
      },
      "source": [
        "Since colab cannot acess internal github hence url is of my github repo. To access an internal copy it is stored at https://github.houston.softwaregrp.net/oo-rnd/busyness-data/blob/master/Results_clean_v2.csv\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "TgnmtAT1iV0T",
        "outputId": "95308d78-4904-4070-c5e8-344f024f864a"
      },
      "source": [
        "raw_data = pd.read_csv(\"https://raw.githubusercontent.com/sashutosh/DeepLearning/master/Data/Results_clean_v2.csv\")\r\n",
        "raw_data.describe()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>averageExecutionTime</th>\n",
              "      <th>medianOfFlowDuration</th>\n",
              "      <th>maxFlowDuration</th>\n",
              "      <th>avgCpuByProcess</th>\n",
              "      <th>medianOfCpuByProcess</th>\n",
              "      <th>maxCpuByProcess</th>\n",
              "      <th>avgDiskByProcess</th>\n",
              "      <th>medianOfDiskByProcess</th>\n",
              "      <th>maxDiskByProcess</th>\n",
              "      <th>avgMemByProcess</th>\n",
              "      <th>medianOfMemByProcess</th>\n",
              "      <th>maxMemByProcess</th>\n",
              "      <th>percentageHeap</th>\n",
              "      <th>medianOfHeap</th>\n",
              "      <th>maxHeap</th>\n",
              "      <th>percentageWorkerThreads</th>\n",
              "      <th>medianOfWorkerThreads</th>\n",
              "      <th>maxWorkerThreads</th>\n",
              "      <th>avgInBufferSize</th>\n",
              "      <th>medianOfInBuffer</th>\n",
              "      <th>maxInBuffer</th>\n",
              "      <th>percentageOutBuffer</th>\n",
              "      <th>medianOfOutBuffer</th>\n",
              "      <th>maxOutBuffer</th>\n",
              "      <th>avgSystemCpuJmx</th>\n",
              "      <th>medianOfSystemCpu</th>\n",
              "      <th>maxSystemCpu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1.001000e+03</td>\n",
              "      <td>1.001000e+03</td>\n",
              "      <td>1.001000e+03</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "      <td>1001.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>18733.003197</td>\n",
              "      <td>19039.875125</td>\n",
              "      <td>21333.276723</td>\n",
              "      <td>0.281965</td>\n",
              "      <td>0.300243</td>\n",
              "      <td>0.401002</td>\n",
              "      <td>1.829535e+09</td>\n",
              "      <td>1.829295e+09</td>\n",
              "      <td>1.830428e+09</td>\n",
              "      <td>47.607412</td>\n",
              "      <td>47.606394</td>\n",
              "      <td>47.615385</td>\n",
              "      <td>0.362306</td>\n",
              "      <td>0.362322</td>\n",
              "      <td>0.466484</td>\n",
              "      <td>29.764607</td>\n",
              "      <td>29.756244</td>\n",
              "      <td>30.847153</td>\n",
              "      <td>0.969633</td>\n",
              "      <td>0.734266</td>\n",
              "      <td>1.919081</td>\n",
              "      <td>1.863504</td>\n",
              "      <td>1.588500</td>\n",
              "      <td>3.883035</td>\n",
              "      <td>0.285370</td>\n",
              "      <td>0.303779</td>\n",
              "      <td>0.405332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>18674.395889</td>\n",
              "      <td>19725.571735</td>\n",
              "      <td>21123.516049</td>\n",
              "      <td>0.191692</td>\n",
              "      <td>0.207350</td>\n",
              "      <td>0.258964</td>\n",
              "      <td>1.463316e+09</td>\n",
              "      <td>1.463231e+09</td>\n",
              "      <td>1.463975e+09</td>\n",
              "      <td>10.888048</td>\n",
              "      <td>10.888017</td>\n",
              "      <td>10.885078</td>\n",
              "      <td>0.177689</td>\n",
              "      <td>0.184013</td>\n",
              "      <td>0.194023</td>\n",
              "      <td>20.652822</td>\n",
              "      <td>20.693055</td>\n",
              "      <td>21.409662</td>\n",
              "      <td>5.315752</td>\n",
              "      <td>4.786159</td>\n",
              "      <td>10.547817</td>\n",
              "      <td>0.953616</td>\n",
              "      <td>1.219071</td>\n",
              "      <td>2.059587</td>\n",
              "      <td>0.194326</td>\n",
              "      <td>0.210422</td>\n",
              "      <td>0.261901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>9073.000000</td>\n",
              "      <td>8923.000000</td>\n",
              "      <td>10178.000000</td>\n",
              "      <td>0.030818</td>\n",
              "      <td>0.016000</td>\n",
              "      <td>0.057000</td>\n",
              "      <td>1.196239e+08</td>\n",
              "      <td>1.200000e+08</td>\n",
              "      <td>1.196239e+08</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>0.076415</td>\n",
              "      <td>0.054000</td>\n",
              "      <td>0.107000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>11897.600000</td>\n",
              "      <td>12114.000000</td>\n",
              "      <td>13252.000000</td>\n",
              "      <td>0.084526</td>\n",
              "      <td>0.093000</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>3.269283e+08</td>\n",
              "      <td>3.270000e+08</td>\n",
              "      <td>3.269346e+08</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>0.207735</td>\n",
              "      <td>0.207000</td>\n",
              "      <td>0.310000</td>\n",
              "      <td>12.166667</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.130000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>2.173000</td>\n",
              "      <td>0.086418</td>\n",
              "      <td>0.095000</td>\n",
              "      <td>0.142000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13210.200000</td>\n",
              "      <td>13401.000000</td>\n",
              "      <td>14704.000000</td>\n",
              "      <td>0.279197</td>\n",
              "      <td>0.296000</td>\n",
              "      <td>0.384000</td>\n",
              "      <td>1.707482e+09</td>\n",
              "      <td>1.710000e+09</td>\n",
              "      <td>1.707485e+09</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>0.320339</td>\n",
              "      <td>0.313000</td>\n",
              "      <td>0.409000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.713333</td>\n",
              "      <td>1.427000</td>\n",
              "      <td>4.027000</td>\n",
              "      <td>0.281380</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.389000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>15934.800000</td>\n",
              "      <td>15906.000000</td>\n",
              "      <td>17641.000000</td>\n",
              "      <td>0.419132</td>\n",
              "      <td>0.457000</td>\n",
              "      <td>0.587000</td>\n",
              "      <td>2.583467e+09</td>\n",
              "      <td>2.580000e+09</td>\n",
              "      <td>2.583467e+09</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>0.509635</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>0.618000</td>\n",
              "      <td>44.833333</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.506667</td>\n",
              "      <td>2.240000</td>\n",
              "      <td>4.640000</td>\n",
              "      <td>0.423855</td>\n",
              "      <td>0.465000</td>\n",
              "      <td>0.598000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>233506.200000</td>\n",
              "      <td>233401.000000</td>\n",
              "      <td>235501.000000</td>\n",
              "      <td>0.809707</td>\n",
              "      <td>0.906000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.311979e+09</td>\n",
              "      <td>6.310000e+09</td>\n",
              "      <td>6.318758e+09</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>0.881175</td>\n",
              "      <td>0.881000</td>\n",
              "      <td>0.934000</td>\n",
              "      <td>91.333333</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>93.000000</td>\n",
              "      <td>93.000000</td>\n",
              "      <td>146.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>6.253000</td>\n",
              "      <td>12.453000</td>\n",
              "      <td>0.816411</td>\n",
              "      <td>0.909000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       averageExecutionTime  ...  maxSystemCpu\n",
              "count           1001.000000  ...   1001.000000\n",
              "mean           18733.003197  ...      0.405332\n",
              "std            18674.395889  ...      0.261901\n",
              "min             9073.000000  ...      0.000000\n",
              "25%            11897.600000  ...      0.142000\n",
              "50%            13210.200000  ...      0.389000\n",
              "75%            15934.800000  ...      0.598000\n",
              "max           233506.200000  ...      1.000000\n",
              "\n",
              "[8 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12sy7ID2lKlN"
      },
      "source": [
        "The raw data contain columns which have all the discreet values (per metric) captured during the experiment. Those values were needed only for manual verification, hence filtering them out.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e8RlZ8NinQS"
      },
      "source": [
        "ras_metrics=raw_data[['averageExecutionTime','avgCpuByProcess','medianOfCpuByProcess','maxCpuByProcess','avgDiskByProcess','medianOfDiskByProcess','maxDiskByProcess','avgMemByProcess','medianOfMemByProcess','maxMemByProcess','percentageHeap','medianOfHeap','maxHeap','percentageWorkerThreads','medianOfWorkerThreads','maxWorkerThreads','avgInBufferSize','medianOfInBuffer','maxInBuffer','percentageOutBuffer','medianOfOutBuffer','maxOutBuffer','avgSystemCpuJmx','medianOfSystemCpu','maxSystemCpu']]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LelMQc11ljrI"
      },
      "source": [
        "### Output variable\r\n",
        "The average execution time is considered as the output variable that directly determines the busyness value of RAS worker node, the higher the execution time more busy\\used the worker node is.\r\n",
        "\r\n",
        "####Capping outliers\r\n",
        "Since there are many outliers in the average execution time capping it to certain maxvalue\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p8EGRzxjAEK",
        "outputId": "cd4a9c25-9571-4d52-f05d-d1e7e45adf2d"
      },
      "source": [
        "#Capping at max value\r\n",
        "maxVal=50000\r\n",
        "ras_metrics['averageExecutionTime'][ras_metrics['averageExecutionTime'] >= maxVal] = maxVal"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:1021: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycxo-Vx1mRcL"
      },
      "source": [
        "### Parameter Normalization\r\n",
        "Except disk usage all other patameters are normalized between 0-1. Using a minmax scaler to normalize disk\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x3td_1IakkNg",
        "outputId": "d5285034-e5a3-49e5-f421-6e66f5d82df2"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "min_max_scaler = MinMaxScaler()\r\n",
        "ras_metrics[['avgDiskByProcess']] = min_max_scaler.fit_transform(ras_metrics[['avgDiskByProcess']])\r\n",
        "ras_metrics[['percentageWorkerThreads']] = min_max_scaler.fit_transform(ras_metrics[['percentageWorkerThreads']])\r\n",
        "fig = px.histogram(ras_metrics, x='avgDiskByProcess',title='Distribution of disk usage',height=400)\r\n",
        "fig.show()\r\n",
        "\r\n",
        "fig1 = px.histogram(ras_metrics, x='percentageWorkerThreads',title='Percentage fo worker threads',height=400)\r\n",
        "fig1.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1734: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1734: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"fe141f6a-2f06-495c-adac-a8924e49a4ae\" class=\"plotly-graph-div\" style=\"height:400px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"fe141f6a-2f06-495c-adac-a8924e49a4ae\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'fe141f6a-2f06-495c-adac-a8924e49a4ae',\n",
              "                        [{\"alignmentgroup\": \"True\", \"bingroup\": \"x\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"avgDiskByProcess=%{x}<br>count=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"histogram\", \"x\": [0.032846172281104605, 0.03284699681386591, 0.03284785500102563, 0.03284874684258378, 0.032849742457253375, 0.03285063990249494, 0.032851502304528934, 0.032852336639699, 0.032853248118372536, 0.03285410349561607, 0.0328814090176129, 0.032882103131505844, 0.03288277621947425, 0.03288349137544069, 0.032884248599405155, 0.03288500582336962, 0.03288572097933605, 0.03288647820330051, 0.032993737064216766, 0.03299441015218518, 0.032995167376149645, 0.03299584046411806, 0.03299638734809239, 0.03299701836806278, 0.032997649388033165, 0.032998343518075055, 0.032999037631968, 0.03299979485593246, 0.03300052402918194, 0.03301001616828195, 0.033188220035218347, 0.033188780936475704, 0.033189285752452016, 0.033189874704424374, 0.03319050572439476, 0.03321802094042396, 0.03321860989239632, 0.03321932504836275, 0.033219857915054066, 0.03322050295230748, 0.03327484753873204, 0.03327547855870243, 0.03327606751067479, 0.033276698530645174, 0.03333037002910133, 0.03333095898107369, 0.03333149184776499, 0.03333256862702501, 0.03335986712423138, 0.033360371940207675, 0.03336100296017806, 0.03336178122621601, 0.03336239120411291, 0.03341623097456117, 0.033416988198525635, 0.03341745094650392, 0.0334179136944822, 0.03344508598769448, 0.03344563287166881, 0.033446305959637226, 0.033446936979607614, 0.03344784144963964, 0.033448514537608054, 0.033449292787497045, 0.03345011312153302, 0.03345080723542597, 0.03345150837410939, 0.03345228802895649, 0.03345309573451857, 0.0334539539216783, 0.033454761627240384, 0.03345558196127636, 0.03345641629644641, 0.03345736984311798, 0.033458253271076516, 0.03345917035343347, 0.033460045367792415, 0.03346081941895608, 0.03346170284691463, 0.03346258627487317, 0.03346340660890913, 0.03346418485879814, 0.03346498415076063, 0.03346574137472509, 0.033466498598689554, 0.033467297890652045, 0.03346819533589361, 0.033469043720644576, 0.0334698850806051, 0.03347062126249607, 0.03347153834485303, 0.033472362877614334, 0.03347314534237761, 0.03347397828873852, 0.03347478599430061, 0.03347562735426113, 0.033476510782219665, 0.03347747834617426, 0.03347831970613477, 0.033479161066095275, 0.0334800024260558, 0.03348080171801829, 0.033481601009980766, 0.03348240030194326, 0.03348324166190378, 0.03348417557145994, 0.03348492438182479, 0.033485681605789255, 0.033486531379349385, 0.03348732225771227, 0.033488163617672775, 0.03348908911362934, 0.03349004826398433, 0.0334908139015484, 0.03349165526150891, 0.03349247859924838, 0.03349335902350342, 0.0334940952053944, 0.03349481036136083, 0.033495525517327276, 0.03349628274129174, 0.0334970399652562, 0.03349789534249974, 0.0334986806171792, 0.033499454668342865, 0.033500342311175685, 0.03350103642506863, 0.03350175158103508, 0.03350261398306907, 0.033503364182243076, 0.03358412664783164, 0.03367547757200198, 0.03377214235450411, 0.03388595941445077, 0.03395995755589308, 0.03404724735988088, 0.03414117247410467, 0.03426048849157445, 0.034362556888236806, 0.03445662924852817, 0.03545306307328018, 0.03786675630191631, 0.04111718706590252, 0.04593046075144401, 0.051533958153993434, 0.05902738577013471, 0.0672496732977876, 0.07569311284012557, 0.08521997076387179, 0.09398973276812947, 0.10320697744498449, 0.11220054969308307, 0.12104086352503154, 0.13068767575324255, 0.14003371610353396, 0.14920195025462463, 0.158215401884568, 0.16833204566367335, 0.17874393718697215, 0.18904895451719078, 0.19954764806563655, 0.21030219029615552, 0.2213270421032552, 0.23195425879556553, 0.24243608364201602, 0.25378643565527514, 0.2652448171570382, 0.2769111683069909, 0.2885098810601641, 0.299809780220912, 0.3116507418233226, 0.32356157150715276, 0.33546086357824234, 0.34705100431067387, 0.3582477839077239, 0.37020470526036214, 0.3817826504720916, 0.394406854732392, 0.40587436329408755, 0.4182355534245589, 0.43124154664876096, 0.4444652807624446, 0.45823295780485734, 0.46982837778843456, 0.4826416017190809, 0.4963259479517691, 0.5097015809666388, 0.522114058688515, 0.5362430039092434, 0.5506230621343402, 0.5635191303070409, 0.5764252295575358, 0.5904736745553705, 0.6047896400457401, 0.6193613402066414, 0.6335818699621579, 0.6472724254636526, 0.6625452186064527, 0.676797686773645, 0.6924948746381935, 0.6986941357828449, 0.6986947948212324, 0.6987753160377529, 0.6987757788664759, 0.6987764939416976, 0.698777167029666, 0.6988043813908763, 0.6988048020708566, 0.6988053068868328, 0.6988058538515519, 0.6988596936220002, 0.6988603245612258, 0.6988607452412061, 0.6988878754664204, 0.6988884644183927, 0.6988890533703651, 0.6988896423223374, 0.6988903575590486, 0.698890862375025, 0.6989185815522115, 0.6989191283554411, 0.6989197594561563, 0.6989203622962201, 0.6989475627693389, 0.6989485724012915, 0.6989761652937394, 0.6989767963944544, 0.6989773431976841, 0.0, 0.000882892233484929, 0.0009316761572729614, 0.0009340682033917962, 0.0009350920625637062, 0.0009358995097426999, 0.0009368758425736534, 0.0009383829104353605, 0.0009392777556970142, 0.0009400349796614763, 0.0009408973655465301, 0.0009416966575090183, 0.0009425403267684616, 0.0009432624913764155, 0.0009440758006219289, 0.0009449171605824432, 0.0009458005885409834, 0.0009466840164995202, 0.0009474833084620084, 0.0009483246684225226, 0.0009492641816621142, 0.0009501504195368361, 0.0009509328843001139, 0.0009517882615436568, 0.0009527081538167961, 0.0009534990321796825, 0.0009544665961342715, 0.0009553500240928117, 0.0009561072480572738, 0.0009569906760158141, 0.0009577058319822501, 0.0009585051239447383, 0.000959346483905249, 0.0009600406139471454, 0.0009607767958381246, 0.0009616186887137744, 0.0009623759126782365, 0.0009633449138888063, 0.0009642002911323493, 0.00096504165109286, 0.000965939096334429, 0.0009668421614083614, 0.0009677398165861932, 0.0009685980037459178, 0.0009694561909056425, 0.0009701797604716837, 0.0009708949164381198, 0.0009716942084006079, 0.0009729168449190417, 0.000973800272877582, 0.008922176751148345, 0.023004965465060246, 0.037666884444591056, 0.05167121036707595, 0.06723816772455099, 0.08112899008653093, 0.09538579792703358, 0.11130520462162902, 0.1262226369827175, 0.14074950048996096, 0.15623536424775936, 0.1718660170113537, 0.18872087484027333, 0.20624448400029488, 0.22118426412540323, 0.23733604103746936, 0.2539090350436557, 0.27021796943740567, 0.28729946590074057, 0.30329715153104037, 0.32150045163952373, 0.3393162694740464, 0.3557657369810013, 0.3725733940316678, 0.3901048790314733, 0.4073384090781835, 0.4240154676842461, 0.44055191097898083, 0.4574918971565348, 0.4757748189853812, 0.4929563109992784, 0.3965155350152477, 0.3966808553034429, 0.3968635809230285, 0.3970773953583681, 0.39721139832325764, 0.3972120504175994, 0.3972127866479372, 0.3972134807295323, 0.3972142169598701, 0.3972149110414651, 0.39721558412943353, 0.39721625721740195, 0.39721693030537036, 0.3972176455420815, 0.3972449440392879, 0.39724557497851354, 0.39724620607922867, 0.39724686511761614, 0.39724755225516545, 0.3972482673303872, 0.39724894041835557, 0.397249613506324, 0.3972503287430352, 0.3972510719174187, 0.3972517450053871, 0.39725230585819765, 0.3972535254264168, 0.3972541563656425, 0.39725474531761484, 0.397282071913983, 0.3972827169027896, 0.3972832638675087, 0.3972852695663013, 0.3972872754265833, 0.39728782222981296, 0.39731503659102324, 0.3973155835557423, 0.39731617250771467, 0.39731676145968703, 0.39731743454765545, 0.39749117279567053, 0.39763827158486675, 0.3976388324376773, 0.39763946353839236, 0.39766697229484416, 0.3976675612468165, 0.39766798192679675, 0.3976685708787691, 0.39766924396673753, 0.39769651436478204, 0.3976971314159162, 0.39769776251663136, 0.39769886807330884, 0.397805958662233, 0.39780688415818954, 0.39780734698691256, 0.3978612287446141, 0.39786179670295985, 0.39786263095738517, 0.39786350041650753, 0.39786442591246407, 0.39786535140842066, 0.3978660666451318, 0.39786686585634956, 0.39786762308031404, 0.3978684644402745, 0.3978692497634008, 0.3978698947522074, 0.3978706519761719, 0.39787149333613236, 0.3978722927088396, 0.39787311598198327, 0.3978740174160139, 0.39787480290062965, 0.39787551797585136, 0.39787623321256255, 0.39787701143015364, 0.39787774766049144, 0.3978786100140786, 0.39787937143676844, 0.39788012446200755, 0.39788090267959864, 0.3978816599035631, 0.3978823751402743, 0.3978831323642387, 0.39788391058182987, 0.3978846046634249, 0.3978854319738045, 0.3978862663897193, 0.3978870446073104, 0.39788784398001764, 0.39788862219760873, 0.3978892742919505, 0.39789001036079885, 0.39789074659113666, 0.3978915879510972, 0.39789242931105767, 0.39789321027396923, 0.397894009485187, 0.3978946825731554, 0.3978954397971199, 0.3978962979519817, 0.3978971646657836, 0.3978978797410054, 0.3978986791137126, 0.3978995624609264, 0.3979004319200487, 0.3979011400512247, 0.3979019182688158, 0.39790275962877636, 0.3979036219823635, 0.39790433721907464, 0.3979050944430391, 0.3979058306733769, 0.3979065878973414, 0.3979073451213058, 0.3979080601965276, 0.397908733284496, 0.39790955365082986, 0.3979102477324249, 0.3979110471051321, 0.3979118631112512, 0.39791270447121174, 0.3979135626260735, 0.3979143703962314, 0.3979152327498186, 0.3979159689801564, 0.39791675430328266, 0.3979175255768281, 0.3979184089240419, 0.3979191661480063, 0.39791983923597474, 0.3979205964599392, 0.39792143781989975, 0.39792211090786817, 0.39792295226782864, 0.3979237726341625, 0.3988777114020759, 0.4009377836877349, 0.40443973878583145, 0.40877962187095196, 0.41420690909054353, 0.41995273502969616, 0.4277171708418664, 0.43528267847637836, 0.4440532580693516, 0.45266323431905986, 0.4612802653962605, 0.46996829869518836, 0.478470636099338, 0.4870891236497601, 0.4951049200296038, 0.5036387934137551, 0.5131226477827272, 0.5222072464904217, 0.5325897356321446, 0.542210267554042, 0.5522055733387498, 0.5620945136221003, 0.5719005952908365, 0.5837338244560821, 0.5944314494119748, 0.6044422016612168, 0.6137086117968009, 0.6236267194945981, 0.633252198160893, 0.6433282546386094, 0.6530364126662447, 0.6639615917401642, 0.6753306965526661, 0.6859625924013787, 0.6964473841317437, 0.7072909049420253, 0.7182933116554221, 0.7299056726882619, 0.7416500595708286, 0.7524631996965323, 0.7631415234350583, 0.7741808840990279, 0.7857089700934121, 0.796269258457992, 0.8076844592995167, 0.8188710984904246, 0.8320501983774583, 0.8444832162636527, 0.8575144581996526, 0.8682279525609118, 0.8796907475689553, 0.8915684078071406, 0.9032138735283615, 0.9158993857995581, 0.9294092053928057, 0.9420391673974501, 0.9571828248669556, 0.9723170794525723, 0.9854596646402985, 1.0, 0.5009933198239616, 0.5011448421317831, 0.5011899190009766, 0.5012440111794131, 0.5013180303467799, 0.5014053202315124, 0.5015192353639935, 0.5016197958046021, 0.5017003451202844, 0.5017542971258906, 0.5018668530017905, 0.501968828897304, 0.502022895721899, 0.5021036412472457, 0.502157726320147, 0.5022117204744959, 0.5023349293240071, 0.5023996337807171, 0.5024892474984307, 0.5026782754047379, 0.5027216417778274, 0.50278900451214, 0.5028829506684372, 0.5029371510447956, 0.5029647114778669, 0.5030543266489855, 0.1757610199553263, 0.17737127827459487, 0.17978097664216397, 0.18285398592323235, 0.18635512178542107, 0.18915039733607675, 0.19125182012219819, 0.1937727130097234, 0.19687611347218312, 0.20022436841592683, 0.20367805577506792, 0.20676857147991354, 0.20997577751319438, 0.2134782711785593, 0.21750995748907684, 0.22178181686362397, 0.22593844545125663, 0.23001040044450613, 0.23456837388224733, 0.2387064311847631, 0.24283259656671602, 0.24730050734611203, 0.25163039988728686, 0.2572293582901153, 0.26182081979215815, 0.2663007780064437, 0.27115575183322504, 0.27576405103129153, 0.2798435085007494, 0.2848533439139059, 0.28997439694000776, 0.2956213942512611, 0.3005631327991298, 0.30971299940017577, 0.3146594521476543, 0.3199157572594316, 0.32593680592975965, 0.3314889357874845, 0.33789285007178266, 0.3430141009224434, 0.34951791773938884, 0.3545373578982289, 0.36015630304107904, 0.3661159684464338, 0.37709348527755926, 0.3821179734346729, 0.3883777688943336, 0.3939110153078622, 0.5944285456704297, 0.5964217067462385, 0.5991851036363097, 0.6035436871980958, 0.607195420033639, 0.6101075474021657, 0.613320322721619, 0.6168857530188401, 0.6205511693387469, 0.6246627472184747, 0.6284045632215215, 0.6324307818227217, 0.6361257381132037, 0.6398730358751487, 0.6438377221566264, 0.6490181437055005, 0.6531631224452347, 0.658079950763163, 0.661741086482595, 0.6666863392020761, 0.6717757819630358, 0.6763486636210216, 0.6816073905898676, 0.6863824709095656, 0.692897968095913, 0.6986034102103658, 0.7037204883354949, 0.7098924406476161, 0.7153907812810861, 0.7201854735238252, 0.7252365828914515, 0.7310507371102534, 0.7371363247106197, 0.7429544176567607, 0.7495635735482097, 0.7558114111996178, 0.7620926611767822, 0.7692171054729614, 0.7748833902749794, 0.78123158408286, 0.7875507566242331, 0.7923979012816775, 0.799690429919235, 0.818092933706354, 0.8241248284916587, 0.8306840226199707, 0.836164486857366, 0.844144942404611, 0.8501469151353354, 0.8575043873954573, 0.8641856625310609, 0.8709802856482469, 0.8786652475028879, 0.8860470412024671, 0.8932850574660763, 0.9017302161442031, 0.9096283736066656, 0.9168044589946247, 0.0007783268387194231, 0.0007791681986799373, 0.000781456471687321, 0.0007821576103707317, 0.0007828306983391417, 0.000783537440705976, 0.0007843451462680659, 0.0007849972083119362, 0.0007857333902029154, 0.0007864485461693514, 0.0007871847442092739, 0.000787941968173736, 0.0007888043540587898, 0.000789645714019304, 0.0007904450059817922, 0.0007912022299462543, 0.0007919734711937416, 0.0007927166778751785, 0.0007935790799091756, 0.0007943993977962034, 0.0007952828257547437, 0.0007961241857152579, 0.000796944519751229, 0.0007976807016422081, 0.0007984799936046963, 0.0007994475575592852, 0.000800431948713088, 0.0008012144134763657, 0.0008021924903932262, 0.0008031411277859418, 0.0008039656605472456, 0.0008048266899210385, 0.0008055839138855006, 0.0008064252738460148, 0.000807266633806529, 0.0008080869678425001, 0.0008088652177315019, 0.0008098385953057841, 0.0008107640912623469, 0.0008116475192208872, 0.0008124748618983726, 0.0008133092132173725, 0.0008140453951083516, 0.0008149555172706283, 0.0008156991922714275, 0.0008164227618374688, 0.0008171589598773878, 0.0008178951417683669, 0.0008190897275718034, 0.0008199507246477097, 0.0008208666119828463, 0.0008217680621624181, 0.0008226262493221427, 0.0008249571555405816, 0.0008258465909061312, 0.0008268081474537108, 0.0008277168162110667, 0.0008285539774462482, 0.0008293883126163047, 0.0008301735872957641, 0.0008310570152543044, 0.0008319572704120551, 0.0008328238711713815, 0.0008656914446138822, 0.0010049512782133105, 0.0011512355308442995, 0.0012690204831222116, 0.0014078419697971072, 0.001543875858129127, 0.0016699043131894464, 0.0018092883967607797, 0.001985921439806221, 0.0020826479274217713, 0.0020940553150814133, 0.0020948826577588987, 0.00218229165709926, 0.0023095195749439773, 0.0024434669390207553, 0.002524218181093534, 0.0025250258866556276, 0.002525811161335087, 0.0025265403345845518, 0.002617941740352514, 0.0027305761647139944, 0.0028552046517064285, 0.0029611700952845398, 0.0029834031372092465, 0.0029842765366738244, 0.0029850477940702516, 0.002985924213387281, 0.003607485295347282, 0.005494370964754969, 0.008406360485899267, 0.012022570361058204, 0.015435791342099988, 0.018803491148063563, 0.021962558068724585, 0.02470414111114849, 0.0278159137996199, 0.030659730006377502, 0.0333679189067556, 0.0360811606823883, 0.03998066131708418, 0.04411461719165423, 0.047768860832647364, 0.05180186872426064, 0.055473791017514304, 0.05986734608555927, 0.0643750395290813, 0.0685007937589307, 0.07256183849359973, 0.07587028782556861, 0.08035094427172769, 0.08448440939605127, 0.08914778262832382, 0.09455578053232014, 0.09809178147922959, 0.1026841810249582, 0.10808506210566779, 0.11375967483714527, 0.11927878317354848, 0.12452702269545482, 0.13040149846693436, 0.13628160190301736, 0.14181971153860712, 0.14712071187159428, 0.1532200131548487, 0.15865035268626634, 0.16573565694938036, 0.1736363016721331, 0.17989690441748338, 0.18621890318547238, 0.19323605454646436, 0.2142850363876677, 0.22091386702154833, 0.22788064918155682, 0.23455777210078715, 0.24043907330401543, 0.24929758631712284, 0.24941603720338718, 0.24958116677255865, 0.2497528264900456, 0.2499386724085097, 0.25017781323206567, 0.2504544359151601, 0.25078822101384435, 0.25097565065920535, 0.250976744427154, 0.250977848691916, 0.25105881416587433, 0.25105973368672174, 0.2510608300385014, 0.2511714047620029, 0.25152902537843225, 0.2517145770631443, 0.25186391619528337, 0.2518648718897644, 0.2518660459179627, 0.2518672096108371, 0.2518682583232332, 0.25186928894881244, 0.25187034072950776, 0.25189816083759176, 0.25189915367464305, 0.2518999108986075, 0.251900668122572, 0.25190146733378976, 0.2519023086937503, 0.25198308231825883, 0.25211703683631764, 0.25219767028799595, 0.2521984275119604, 0.2522191537121086, 0.252359694415317, 0.25249894578687004, 0.25262756808524817, 0.25275082312073804, 0.2527889737080301, 0.2527897982730893, 0.2528771133501738, 0.2530469839262015, 0.25313833074854014, 0.2531391089661313, 0.2531399083388385, 0.2531407243449576, 0.2531415910587595, 0.2531425165547161, 0.25314344205067263, 0.2531443675466292, 0.253145166757847, 0.25314592398181146, 0.25314676534177194, 0.2531476488504752, 0.2531485743464318, 0.253176230381249, 0.25317724695724736, 0.2532225201975948, 0.25355568340046175, 0.2536593789969771, 0.2536869299021716, 0.253741022080608, 0.25392832544123023, 0.25400945805675457, 0.254112475559129, 0.25435743936876104, 0.25438499027395556, 0.25450009073889535, 0.25470643031272017, 0.25473402320516797, 0.25481490906483445, 0.2549223361977428, 0.2549793348630447, 0.2551267926432567, 0.2553246620961738, 0.255325461468881, 0.25532645301401685, 0.25532735444804744, 0.25532824845356406, 0.2553293317246994, 0.25533042549264806, 0.25533140977075924, 0.25533236062055725, 0.2553334753821326, 0.2553877148389344, 0.255388640334891, 0.2553895236821048, 0.25539042107889953, 0.2553914693068273, 0.2553924684419666, 0.2554438852249201, 0.25547465348414405, 0.2555022780285213, 0.25552982893371584, 0.2555574639749064, 0.25555859973010836, 0.2555597356467998, 0.25558748631442635, 0.25563284013800225, 0.25585568603152836, 0.2559100727666957, 0.25593813446297564, 0.25596618420903733, 0.25614376590562443, 0.25639447292555123, 0.25642230772917385, 0.25657330616526575, 0.25658433185649493, 0.2566121351696776, 0.25666666676086886, 0.2566945018874704, 0.25682357425690605, 0.25722941142013966, 0.25745237164818635, 0.2576841769756106, 0.25771178181827664, 0.25776603031848694, 0.25779371800523354, 0.25782159495759893, 0.25800458961858463, 0.25835730725425365, 0.25838546406781127, 0.25841349588854556, 0.2587135306640844, 0.2590080986992426, 0.2591300435709398, 0.2591627934670305, 0.25916357168462156, 0.2591643920509554, 0.25924515162588313, 0.2594123597255828, 0.2595308070590795, 0.25968955860298415, 0.25972765864408276, 0.2597284790104166, 0.2597292362343811, 0.25972988832872285, 0.25973056141669126, 0.2597313942177117, 0.25973220198786956, 0.25973300136057675, 0.25976038399377915, 0.259761267340993, 0.2597621508496963, 0.2597629219617522, 0.25976356711204823, 0.25983352223627376, 0.2600207414608999, 0.26011210927686523, 0.26011297873598754, 0.26018033452625444, 0.2602742596889251, 0.2602749747641468, 0.26027573198811127, 0.2602766154968145, 0.2602774778504017, 0.2602782896577955, 0.26027911842157997, 0.26030649685605706, 0.26030732142111623, 0.26030816278107677, 0.2603091304257761, 0.2603100559217326, 0.2603380145877525, 0.26036582662136476, 0.2603666216338572, 0.2603674883476591, 0.26039495511685756, 0.26039569829124115, 0.26042303183165505, 0.2605302556493638, 0.2606685915373052, 0.2607715179596378, 0.2608521514113161, 0.2609328648002652, 0.2609337902962217, 0.260934757940921, 0.26093579276522566, 0.26093656678409144, 0.26093736615679863, 0.26093820751675917, 0.260939090863973, 0.26094003040951047, 0.26094087451479137, 0.2609576742607976, 0.2611821691229526, 0.26118301048291315, 0.26118380985562034, 0.2611846090668381, 0.26123874339401726, 0.26126633628646506, 0.26137376341937346, 0.2614016121110876, 0.261578716283413, 0.2617235537360397, 0.2617776669081028, 0.26177857738554183, 0.2618061223156272, 0.2618070757492562, 0.26183455656803556, 0.261862023337234, 0.26205695804263435, 0.2624096532312718, 0.2626101430117742, 0.26271752815742927, 0.2628782061088135, 0.2629058411500041, 0.2630666311750568, 0.2632008240515228, 0.2632604516033501, 0.2634690132718115, 0.26360306581395776, 0.26372645973036246, 0.2639402706129345, 0.26412045987944144, 0.26442209727614113, 0.2646726255272625, 0.26470005014771825, 0.26475426861089346, 0.26494614971255226, 0.26515579513661724, 0.2652368072810225, 0.26545603581118676, 0.26578439502631757], \"xaxis\": \"x\", \"yaxis\": \"y\"}],\n",
              "                        {\"barmode\": \"relative\", \"height\": 400, \"legend\": {\"tracegroupgap\": 0}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Distribution of disk usage\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"avgDiskByProcess\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"count\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('fe141f6a-2f06-495c-adac-a8924e49a4ae');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"a53a9029-71b4-4272-ba84-f24c079eb5d2\" class=\"plotly-graph-div\" style=\"height:400px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"a53a9029-71b4-4272-ba84-f24c079eb5d2\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'a53a9029-71b4-4272-ba84-f24c079eb5d2',\n",
              "                        [{\"alignmentgroup\": \"True\", \"bingroup\": \"x\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"percentageWorkerThreads=%{x}<br>count=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"type\": \"histogram\", \"x\": [0.011070110701515503, 0.011070110701515503, 0.02214022140303101, 0.036373220879570976, 0.04428044280606201, 0.055350553507577525, 0.0691881918844719, 0.08302583026136627, 0.09409594096288178, 0.09963099631363953, 0.1134686346905339, 0.12546125458027535, 0.13284132841818605, 0.1494464944704593, 0.16328413284735366, 0.17158671587349028, 0.18450184506215878, 0.19040590406606667, 0.2066420664651898, 0.2177121771667053, 0.22416974170568896, 0.23247232473182558, 0.24907749078409883, 0.25830258299846137, 0.27121771218712987, 0.2795202952132665, 0.2878228782394031, 0.3016605166162975, 0.3136531365060389, 0.32841328418186033, 0.33210332104546514, 0.3431734317469806, 0.36531365315001163, 0.3726937269879223, 0.38191881920228493, 0.3929889299038004, 0.40959409595607366, 0.40959409595607366, 0.424354243521194, 0.43542435422270953, 0.4575645756257405, 0.46494464946365116, 0.4760147601651667, 0.4870848708666822, 0.49815498156819765, 0.5092250922697131, 0.5239852398348335, 0.5369003690235019, 0.5424354243742596, 0.5535055350757752, 0.5645756457772907, 0.5839483395049427, 0.5922509225310794, 0.6033210332325949, 0.6088560885833526, 0.6199261992848681, 0.6420664206878991, 0.6531365313894146, 0.6642066420909302, 0.6697416974416879, 0.011070110701515503, 0.02214022140303101, 0.03321033210454651, 0.04428044280606201, 0.055350553507577525, 0.07011070110590814, 0.08118081180742363, 0.08856088561212402, 0.09963099631363953, 0.11070110701515504, 0.12177121771667054, 0.13284132841818605, 0.1457564576068545, 0.15498154982121704, 0.1678966790098855, 0.17712177122424805, 0.19040590406606667, 0.19926199262727906, 0.21033210332879457, 0.22140221403031007, 0.23247232473182558, 0.24354243543334106, 0.2546125461348566, 0.268450184511751, 0.2795202952132665, 0.2922509225200093, 0.3007380074280716, 0.3099630996424341, 0.3210332103439496, 0.33210332104546514, 0.3431734317469806, 0.3542435424484961, 0.3680811808253905, 0.37638376385152716, 0.3874538745530427, 0.3985239852545582, 0.40959409595607366, 0.4206642066575892, 0.4361623616397109, 0.4472324723412264, 0.4538745387621357, 0.46494464946365116, 0.4760147601651667, 0.4870848708666822, 0.49815498156819765, 0.5092250922697131, 0.5202952029712287, 0.5313653136727441, 0.5247232472518348, 0.5535055350757752, 0.5645756457772907, 0.5044807590961063, 0.42509225093819536, 0.5535055350757752, 0.5036900369189554, 0.4016868740738631, 0.5424354243742596, 0.5673431734526695, 0.5977859778818372, 0.5756457564788061, 0.5867158671803216, 0.6199261992848681, 0.30442804429167636, 0.4501845018985309, 0.5313653136727441, 0.46771217713903007, 0.511992619945092, 0.5701107011280484, 0.47324723248978784, 0.4557195572492886, 0.5380073800936535, 0.4428044280606202, 0.5424354243742596, 0.5092250922697131, 0.4483394834113779, 0.5479704797250174, 0.5894833948557006, 0.5092250922697131, 0.4380600948870134, 0.5645756457772907, 0.011070110701515503, 0.02214022140303101, 0.03321033210454651, 0.04428044280606201, 0.055350553507577525, 0.06642066420909301, 0.07749077491060852, 0.08856088561212402, 0.09963099631363953, 0.11070110701515504, 0.12177121771667054, 0.13284132841818605, 0.14612546126000464, 0.15498154982121704, 0.16826568266303563, 0.17712177122424805, 0.18819188192576358, 0.19926199262727906, 0.21254612546909768, 0.22140221403031007, 0.23247232473182558, 0.24354243543334106, 0.2546125461348566, 0.2656826568363721, 0.27675276753788763, 0.2878228782394031, 0.2988929889409186, 0.3099630996424341, 0.3210332103439496, 0.33210332104546514, 0.3431734317469806, 0.3542435424484961, 0.3680811808253905, 0.37638376385152716, 0.3874538745530427, 0.39188191883364887, 0.40959409595607366, 0.4206642066575892, 0.4345018450344836, 0.4428044280606202, 0.4538745387621357, 0.46771217713903007, 0.4760147601651667, 0.48985239854206103, 0.5009225092435765, 0.5092250922697131, 0.5202952029712287, 0.5313653136727441, 0.5446494465145628, 0.5535055350757752, 0.5645756457772907, 0.5756457564788061, 0.5867158671803216, 0.5977859778818372, 0.6088560885833526, 0.623616236148473, 0.6309963099863837, 0.644833948363278, 0.6559040590647935, 0.6660516605780831, 0.6697416974416879, 0.6752767527924457, 0.6863468634939611, 0.7029520295462344, 0.7084870848969922, 0.7250922509492654, 0.7416974170015387, 0.7416974170015387, 0.7527675277030543, 0.7693726937553276, 0.780442804456843, 0.7859778598076008, 0.8191881919121473, 0.8191881919121473, 0.8376383764515735, 0.8302583026136627, 0.8413284133151783, 0.8523985240166938, 0.8745387454197248, 0.8856088561212403, 0.9022140221735135, 0.9132841328750291, 0.9225092250893916, 0.9409594096288177, 0.9575645756810911, 0.9741697417333643, 0.9852398524348798, 1.0, 0.011070110701515503, 0.02214022140303101, 0.03321033210454651, 0.04428044280606201, 0.055350553507577525, 0.0691881918844719, 0.07749077491060852, 0.08856088561212402, 0.09963099631363953, 0.11070110701515504, 0.12361623620382348, 0.13284132841818605, 0.1494464944704593, 0.15498154982121704, 0.16605166052273254, 0.17712177122424805, 0.18819188192576358, 0.19926199262727906, 0.21033210332879457, 0.22324723251746303, 0.23431734321897854, 0.24354243543334106, 0.2573800738102355, 0.26752767532352506, 0.2789667896781907, 0.2878228782394031, 0.2988929889409186, 0.3136531365060389, 0.3210332103439496, 0.33210332104546514, 0.3431734317469806, 0.3542435424484961, 0.3431734317469806, 0.37638376385152716, 0.3874538745530427, 0.3985239852545582, 0.40959409595607366, 0.4206642066575892, 0.43173431735910467, 0.4428044280606202, 0.3487084870977384, 0.42509225093819536, 0.416236162376983, 0.4715867158845605, 0.4892988930069853, 0.4797047970287715, 0.5202952029712287, 0.5136531365503193, 0.49815498156819765, 0.5147601476204708, 0.6642066420909302, 0.6752767527924457, 0.6863468634939611, 0.6974169741954767, 0.7084870848969922, 0.7232472324621125, 0.7306273063000231, 0.7416974170015387, 0.7527675277030543, 0.7638376384045698, 0.7776752767814642, 0.7859778598076008, 0.7970479705091162, 0.8081180812106318, 0.8191881919121473, 0.8302583026136627, 0.8450184501787831, 0.8523985240166938, 0.8634686347182093, 0.8745387454197248, 0.8883763837966192, 0.8966789668227558, 0.9077490775242713, 0.9210332103660899, 0.9321033210676054, 0.9409594096288177, 0.9520295203303333, 0.9630996310318488, 0.9741697417333643, 0.9852398524348798, 0.9963099631363953, 0.011070110701515503, 0.02214022140303101, 0.03321033210454651, 0.04428044280606201, 0.058118081182956395, 0.06642066420909301, 0.07749077491060852, 0.09225092250893914, 0.10239852398901841, 0.12177121771667054, 0.12177121771667054, 0.13284132841818605, 0.14760147598330634, 0.15498154982121704, 0.17158671587349028, 0.18450184506215878, 0.19372693727652132, 0.2066420664651898, 0.21586715867955233, 0.2287822878682208, 0.2398523985697363, 0.2509225092712518, 0.2546125461348566, 0.2656826568363721, 0.2804428044014924, 0.2988929889409186, 0.3071955719670552, 0.3210332103439496, 0.32841328418186033, 0.33210332104546514, 0.3431734317469806, 0.3708487085007694, 0.36531365315001163, 0.38191881920228493, 0.3874538745530427, 0.3985239852545582, 0.4151291513068314, 0.42619926200834696, 0.43726937270986244, 0.4483394834113779, 0.46494464946365116, 0.468634686327256, 0.4870848708666822, 0.4870848708666822, 0.5092250922697131, 0.5202952029712287, 0.5424354243742596, 0.5461254612378644, 0.5590405904265329, 0.5719557196152013, 0.5867158671803216, 0.6033210332325949, 0.6088560885833526, 0.6199261992848681, 0.6420664206878991, 0.6531365313894146, 0.6642066420909302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0027675276753788753, 0.0027675276753788753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0027675276753788753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0022140221403031007, 0.0, 0.0027675276753788753, 0.0, 0.0027675276753788753, 0.0, 0.0027675276753788753, 0.0, 0.0, 0.005535055350757752, 0.0, 0.0, 0.0, 0.0, 0.0022140221403031007, 0.0, 0.0, 0.0, 0.0, 0.0018450184539426221, 0.0, 0.0, 0.0, 0.0022140221403031007, 0.0036900368968151307, 0.0022140221403031007, 0.0, 0.0027675276753788753, 0.011070110701515503, 0.025830258299846143, 0.03321033210454651, 0.04428044280606201, 0.055350553507577525, 0.06642066420909301, 0.07749077491060852, 0.08856088561212402, 0.09963099631363953, 0.11070110701515504, 0.1245387453920494, 0.13284132841818605, 0.14391143911970153, 0.15774907749659592, 0.16605166052273254, 0.17712177122424805, 0.18819188192576358, 0.19926199262727906, 0.21033210332879457, 0.22140221403031007, 0.23247232473182558, 0.24354243543334106, 0.2546125461348566, 0.2656826568363721, 0.27675276753788763, 0.2878228782394031, 0.2988929889409186, 0.3121771217827372, 0.32287822883110256, 0.3343173431857682, 0.3431734317469806, 0.3542435424484961, 0.36531365315001163, 0.3791512915269061, 0.39022140222842155, 0.3985239852545582, 0.41236162363145257, 0.4206642066575892, 0.43173431735910467, 0.4428044280606202, 0.45664206643751454, 0.46494464946365116, 0.4760147601651667, 0.4892988930069853, 0.49815498156819765, 0.511992619945092, 0.5202952029712287, 0.5313653136727441, 0.5446494465145628, 0.5535055350757752, 0.5645756457772907, 0.5778597786191093, 0.5867158671803216, 0.5977859778818372, 0.6088560885833526, 0.622693726960247, 0.6309963099863837, 0.6442804428282023, 0.6531365313894146, 0.6642066420909302, 0.49815498156819765, 0.5645756457772907, 0.5756457564788061, 0.37638376385152716, 0.5756457564788061, 0.5977859778818372, 0.534132841348123, 0.5719557196152013, 0.6125461254469575, 0.5756457564788061, 0.5690036900578969, 0.5867158671803216, 0.6354243542669898, 0.6116236162587315, 0.6070110700961997, 0.5645756457772907, 0.615498155004262, 0.6365313653371414, 0.5756457564788061, 0.5468634686548658, 0.6282287823110048, 0.622693726960247, 0.6354243542669898, 0.5519240907214733, 0.6243542435654743, 0.667896678954535, 0.011070110701515503, 0.02214022140303101, 0.03321033210454651, 0.04870848708666822, 0.058118081182956395, 0.07195571955985076, 0.08118081180742363, 0.09963099631363953, 0.10239852398901841, 0.11623616236591278, 0.12546125458027535, 0.13284132841818605, 0.15498154982121704, 0.1605166051719748, 0.16974169738633732, 0.18450184506215878, 0.19557195576367428, 0.20479704797803683, 0.21402214019239935, 0.22509225089391485, 0.23616236159543036, 0.2546125461348566, 0.2656826568363721, 0.27121771218712987, 0.28228782288864535, 0.2933579335901609, 0.30442804429167636, 0.3210332103439496, 0.33210332104546514, 0.33763837639622285, 0.3431734317469806, 0.35977859779925386, 0.37638376385152716, 0.3874538745530427, 0.402214022118163, 0.4206642066575892, 0.43173431735910467, 0.43726937270986244, 0.4428044280606202, 0.4538745387621357, 0.46494464946365116, 0.4870848708666822, 0.49815498156819765, 0.5147601476204708, 0.5239852398348335, 0.5424354243742596, 0.5562730627511541, 0.5582498682493819, 0.013284132841818607, 0.02214022140303101, 0.035055350558489136, 0.04797047970287715, 0.055350553507577525, 0.0691881918844719, 0.07749077491060852, 0.09520295203303333, 0.10332103317724434, 0.11439114387875984, 0.1245387453920494, 0.1383763837689438, 0.15055350554061084, 0.15867158668482184, 0.17712177122424805, 0.1826568265750058, 0.19557195576367428, 0.19926199262727906, 0.21586715867955233, 0.22509225089391485, 0.23800738008258332, 0.24354243543334106, 0.2546125461348566, 0.2656826568363721, 0.2841328413757983, 0.2988929889409186, 0.2988929889409186, 0.31549815499319184, 0.33210332104546514, 0.3394833948833758, 0.3468634686105854, 0.3542435424484961, 0.3726937269879223, 0.37638376385152716, 0.3874538745530427, 0.3985239852545582, 0.4206642066575892, 0.42343173433296805, 0.43726937270986244, 0.446494464924225, 0.46494464946365116, 0.4760147601651667, 0.4760147601651667, 0.5092250922697131, 0.5313653136727441, 0.5424354243742596, 0.5557195572160782, 0.5645756457772907, 0.5601476014966844, 0.5661570901315925, 0.578413284154185, 0.5946230890625323, 0.6033210332325949, 0.6118752096535747, 0.6216741115358552, 0.6301447629752821, 0.6341591988056886, 0.6527675277362646, 0.011070110701515503, 0.02214022140303101, 0.03321033210454651, 0.04797047970287715, 0.055350553507577525, 0.06642066420909301, 0.07749077491060852, 0.08856088561212402, 0.09963099631363953, 0.11439114387875984, 0.12177121771667054, 0.13284132841818605, 0.14391143911970153, 0.15719557196152012, 0.17158671587349028, 0.17988929889962693, 0.18819188192576358, 0.19926199262727906, 0.21033210332879457, 0.22324723251746303, 0.23247232473182558, 0.24354243543334106, 0.2546125461348566, 0.2656826568363721, 0.2789667896781907, 0.2878228782394031, 0.2988929889409186, 0.3099630996424341, 0.27675276753788763, 0.33210332104546514, 0.3431734317469806, 0.3542435424484961, 0.36752767529031477, 0.37638376385152716, 0.3874538745530427, 0.3985239852545582, 0.40959409595607366, 0.40590405909246885, 0.40405904060531594, 0.44557195573599906, 0.32287822883110256, 0.4428044280606202, 0.47878228784054555, 0.44557195573599906, 0.4627306273233481, 0.5092250922697131, 0.5092250922697131, 0.5147601476204708, 0.446494464924225, 0.504797047989107, 0.47443331581086484, 0.4472324723412264, 0.50701107012941, 0.5313653136727441, 0.4791776489844716, 0.5092250922697131, 0.3210332103439496, 0.46494464946365116, 0.29520295207731384, 0.4892988930069853, 0.402214022118163, 0.44059040592031706, 0.48154981551592446, 0.5335793358130473, 0.5202952029712287, 0.3210332103439496, 0.4760147601651667, 0.532946758027046, 0.48523985237952927, 0.5690036900578969, 0.35645756458879924, 0.5269372693921379, 0.460516605183045, 0.5064575645943342, 0.3726937269879223, 0.5202952029712287, 0.43173431735910467, 0.47047970481440893, 0.4870848708666822, 0.573431734338503, 0.5461254612378644, 0.5468634686548658, 0.4627306273233481, 0.48487084872637903, 0.49815498156819765, 0.42952029521880153, 0.42857142853979974, 0.46494464946365116, 0.4169741697939844, 0.35977859779925386, 0.011070110701515503, 0.02214022140303101, 0.03321033210454651, 0.04870848708666822, 0.06088560885833528, 0.06642066420909301, 0.08413284133151781, 0.09594095941682441, 0.10332103317724434, 0.11900369004129166, 0.12915129155458124, 0.13653136528179083, 0.14833948340030775, 0.1605166051719748, 0.17712177122424805, 0.17712177122424805, 0.19095940960114244, 0.19926199262727906, 0.21033210332879457, 0.22140221403031007, 0.23247232473182558, 0.2656826568363721, 0.2546125461348566, 0.27121771218712987, 0.2804428044014924, 0.2878228782394031, 0.3099630996424341, 0.31734317348034485, 0.32656826569470737, 0.3542435424484961, 0.3542435424484961, 0.35977859779925386, 0.36531365315001163, 0.38191881920228493, 0.3874538745530427, 0.40590405909246885, 0.4132841328196785, 0.4206642066575892, 0.4345018450344836, 0.4428044280606202, 0.4538745387621357, 0.47047970481440893, 0.4760147601651667, 0.5092250922697131, 0.5258302583219864, 0.5335793358130473, 0.5645756457772907, 0.5590405904265329, 0.011070110701515503, 0.02214022140303101, 0.03321033210454651, 0.04428044280606201, 0.055350553507577525, 0.06642066420909301, 0.07749077491060852, 0.08856088561212402, 0.10101476015132897, 0.11070110701515504, 0.12177121771667054, 0.13284132841818605, 0.14391143911970153, 0.15498154982121704, 0.1672816728106008, 0.17712177122424805, 0.18819188192576358, 0.20084343698158097, 0.21033210332879457, 0.2224085877203841, 0.23247232473182558, 0.24354243543334106, 0.2546125461348566, 0.26706642067406156, 0.27675276753788763, 0.011070110701515503, 0.02214022140303101, 0.03321033210454651, 0.04649446494636512, 0.055350553507577525, 0.06826568266303563, 0.07749077491060852, 0.08856088561212402, 0.09963099631363953, 0.11070110701515504, 0.12177121771667054, 0.13284132841818605, 0.14391143911970153, 0.15719557196152012, 0.16605166052273254, 0.17712177122424805, 0.18819188192576358, 0.19926199262727906, 0.21033210332879457, 0.22140221403031007, 0.23247232473182558, 0.24575645757364414, 0.2546125461348566, 0.2656826568363721, 0.27675276753788763, 0.011070110701515503, 0.02214022140303101, 0.03321033210454651, 0.04428044280606201, 0.055350553507577525, 0.06642066420909301, 0.07887453874829796, 0.09040590406606665, 0.10332103317724434, 0.11386399583445993, 0.12335266207097242, 0.13948339483909533, 0.14612546126000464, 0.15814443864052194, 0.17343173436064327, 0.178966789711401, 0.19372693727652132, 0.20295202949088387, 0.21191354768309648, 0.2269372693810678, 0.23563521355113048, 0.24986821296124975, 0.2590405904154628, 0.2690036900468268, 0.27675276753788763, 0.013837638376894381, 0.02214022140303101, 0.0379546652670832, 0.04428044280606201, 0.055350553507577525, 0.06642066420909301, 0.07749077491060852, 0.09077490775242711, 0.10101476015132897, 0.11070110701515504, 0.12177121771667054, 0.13468634690533898, 0.14391143911970153, 0.15498154982121704, 0.16605166052273254, 0.17712177122424805, 0.18942189421363184, 0.19926199262727906, 0.2177121771667053, 0.22324723251746303, 0.23247232473182558, 0.24354243543334106, 0.2546125461348566, 0.2698339483494404, 0.2785977860250406, 0.289404322593705, 0.2988929889409186, 0.3099630996424341, 0.3221402214141012, 0.33210332104546514, 0.3431734317469806, 0.3582690372087923, 0.36946494466307994, 0.377965208205829, 0.3884602482431167, 0.39975399754242646, 0.4118081180963768, 0.42177121772774073, 0.4348972061784096, 0.446494464924225, 0.4575645756257405, 0.4681075382829561, 0.4791776489844716, 0.48985239854206103, 0.5000000000553506, 0.5106088561074026, 0.5230627306466075, 0.5321560358498951, 0.5435424354444112, 0.5584255842272482, 0.011070110701515503, 0.02214022140303101, 0.03321033210454651, 0.04428044280606201, 0.055350553507577525, 0.06642066420909301, 0.07749077491060852, 0.08856088561212402, 0.10184501845394263, 0.11070110701515504, 0.12177121771667054, 0.1356088560935649, 0.14391143911970153, 0.15498154982121704, 0.16605166052273254, 0.17933579336455113, 0.18819188192576358, 0.19926199262727906, 0.21033210332879457, 0.22140221403031007, 0.23247232473182558, 0.24354243543334106, 0.2546125461348566, 0.2678966789766752, 0.27675276753788763, 0.2915129151030079, 0.2988929889409186, 0.3099630996424341, 0.3210332103439496, 0.3343173431857682, 0.3431734317469806, 0.3542435424484961, 0.36531365315001163, 0.3791512915269061, 0.3874538745530427, 0.3985239852545582, 0.4140221402366799, 0.4206642066575892, 0.43173431735910467, 0.4428044280606202, 0.45664206643751454, 0.46494464946365116, 0.4760147601651667, 0.4870848708666822, 0.5018450184318024, 0.5092250922697131, 0.5239852398348335, 0.5369003690235019, 0.5424354243742596, 0.5535055350757752, 0.011070110701515503, 0.02214022140303101, 0.03321033210454651, 0.04649446494636512, 0.055350553507577525, 0.06642066420909301, 0.07749077491060852, 0.09077490775242711, 0.10147601480079248, 0.11291512915545812, 0.12398523985697363, 0.13284132841818605, 0.14612546126000464, 0.15774907749659592, 0.17047970480333874, 0.18081180808785283, 0.1913547707450685, 0.20369003690788526, 0.21288674425139806, 0.22361623617061316, 0.23800738008258332, 0.24907749078409883, 0.25777543495416155, 0.2693726936999769, 0.2804428044014924, 0.2878228782394031, 0.30110701108122173, 0.31312598846173906, 0.33210332104546514, 0.33874538746637445, 0.3468634686105854, 0.35977859779925386, 0.36669741698770103, 0.38191881920228493, 0.3896678966933458, 0.4007380073948613, 0.4151291513068314, 0.424354243521194, 0.4339483394994078, 0.45166051662183254, 0.4538745387621357, 0.46771217713903007, 0.47878228784054555, 0.4937269372875915, 0.5036900369189554, 0.5129151291333179, 0.5243206977315248, 0.5369003690235019, 0.5468634686548658, 0.5608856089136858], \"xaxis\": \"x\", \"yaxis\": \"y\"}],\n",
              "                        {\"barmode\": \"relative\", \"height\": 400, \"legend\": {\"tracegroupgap\": 0}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Percentage fo worker threads\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"percentageWorkerThreads\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"count\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a53a9029-71b4-4272-ba84-f24c079eb5d2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxHpYKw-mnun"
      },
      "source": [
        "###Initial data analysis\r\n",
        "Checking correlation between the output and input variables "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "UHYHzp3mlFx9",
        "outputId": "15b1910d-0e64-42e3-f0bb-1b56d31f957a"
      },
      "source": [
        "corr = ras_metrics.corr()\r\n",
        "corr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>averageExecutionTime</th>\n",
              "      <th>avgCpuByProcess</th>\n",
              "      <th>medianOfCpuByProcess</th>\n",
              "      <th>maxCpuByProcess</th>\n",
              "      <th>avgDiskByProcess</th>\n",
              "      <th>medianOfDiskByProcess</th>\n",
              "      <th>maxDiskByProcess</th>\n",
              "      <th>avgMemByProcess</th>\n",
              "      <th>medianOfMemByProcess</th>\n",
              "      <th>maxMemByProcess</th>\n",
              "      <th>percentageHeap</th>\n",
              "      <th>medianOfHeap</th>\n",
              "      <th>maxHeap</th>\n",
              "      <th>percentageWorkerThreads</th>\n",
              "      <th>medianOfWorkerThreads</th>\n",
              "      <th>maxWorkerThreads</th>\n",
              "      <th>avgInBufferSize</th>\n",
              "      <th>medianOfInBuffer</th>\n",
              "      <th>maxInBuffer</th>\n",
              "      <th>percentageOutBuffer</th>\n",
              "      <th>medianOfOutBuffer</th>\n",
              "      <th>maxOutBuffer</th>\n",
              "      <th>avgSystemCpuJmx</th>\n",
              "      <th>medianOfSystemCpu</th>\n",
              "      <th>maxSystemCpu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>averageExecutionTime</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.280996</td>\n",
              "      <td>0.261092</td>\n",
              "      <td>0.287648</td>\n",
              "      <td>0.198137</td>\n",
              "      <td>0.198296</td>\n",
              "      <td>0.198136</td>\n",
              "      <td>0.194608</td>\n",
              "      <td>0.194459</td>\n",
              "      <td>0.194683</td>\n",
              "      <td>0.392303</td>\n",
              "      <td>0.378232</td>\n",
              "      <td>0.382723</td>\n",
              "      <td>0.363906</td>\n",
              "      <td>0.360850</td>\n",
              "      <td>0.340872</td>\n",
              "      <td>0.259144</td>\n",
              "      <td>0.224459</td>\n",
              "      <td>0.253975</td>\n",
              "      <td>-0.011664</td>\n",
              "      <td>-0.037234</td>\n",
              "      <td>-0.008066</td>\n",
              "      <td>0.279643</td>\n",
              "      <td>0.259809</td>\n",
              "      <td>0.286002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>avgCpuByProcess</th>\n",
              "      <td>0.280996</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.990295</td>\n",
              "      <td>0.982352</td>\n",
              "      <td>0.352586</td>\n",
              "      <td>0.352789</td>\n",
              "      <td>0.352981</td>\n",
              "      <td>0.307152</td>\n",
              "      <td>0.307037</td>\n",
              "      <td>0.307333</td>\n",
              "      <td>0.304237</td>\n",
              "      <td>0.298966</td>\n",
              "      <td>0.430460</td>\n",
              "      <td>0.229928</td>\n",
              "      <td>0.224596</td>\n",
              "      <td>0.192679</td>\n",
              "      <td>0.110596</td>\n",
              "      <td>0.102793</td>\n",
              "      <td>0.107572</td>\n",
              "      <td>0.340130</td>\n",
              "      <td>0.171820</td>\n",
              "      <td>0.449334</td>\n",
              "      <td>0.997970</td>\n",
              "      <td>0.988611</td>\n",
              "      <td>0.980527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medianOfCpuByProcess</th>\n",
              "      <td>0.261092</td>\n",
              "      <td>0.990295</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.960539</td>\n",
              "      <td>0.346600</td>\n",
              "      <td>0.346805</td>\n",
              "      <td>0.347003</td>\n",
              "      <td>0.314259</td>\n",
              "      <td>0.314168</td>\n",
              "      <td>0.314398</td>\n",
              "      <td>0.296399</td>\n",
              "      <td>0.290886</td>\n",
              "      <td>0.421043</td>\n",
              "      <td>0.202776</td>\n",
              "      <td>0.197404</td>\n",
              "      <td>0.165443</td>\n",
              "      <td>0.112358</td>\n",
              "      <td>0.099449</td>\n",
              "      <td>0.111570</td>\n",
              "      <td>0.328117</td>\n",
              "      <td>0.165945</td>\n",
              "      <td>0.431969</td>\n",
              "      <td>0.988494</td>\n",
              "      <td>0.998051</td>\n",
              "      <td>0.959106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maxCpuByProcess</th>\n",
              "      <td>0.287648</td>\n",
              "      <td>0.982352</td>\n",
              "      <td>0.960539</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.354238</td>\n",
              "      <td>0.354423</td>\n",
              "      <td>0.354647</td>\n",
              "      <td>0.292993</td>\n",
              "      <td>0.292843</td>\n",
              "      <td>0.293270</td>\n",
              "      <td>0.295579</td>\n",
              "      <td>0.292176</td>\n",
              "      <td>0.421924</td>\n",
              "      <td>0.222130</td>\n",
              "      <td>0.216752</td>\n",
              "      <td>0.186833</td>\n",
              "      <td>0.119677</td>\n",
              "      <td>0.114637</td>\n",
              "      <td>0.118968</td>\n",
              "      <td>0.318787</td>\n",
              "      <td>0.154946</td>\n",
              "      <td>0.433875</td>\n",
              "      <td>0.981390</td>\n",
              "      <td>0.960049</td>\n",
              "      <td>0.998535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>avgDiskByProcess</th>\n",
              "      <td>0.198137</td>\n",
              "      <td>0.352586</td>\n",
              "      <td>0.346600</td>\n",
              "      <td>0.354238</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.414446</td>\n",
              "      <td>0.414420</td>\n",
              "      <td>0.414515</td>\n",
              "      <td>0.163432</td>\n",
              "      <td>0.153003</td>\n",
              "      <td>0.148056</td>\n",
              "      <td>0.203379</td>\n",
              "      <td>0.200188</td>\n",
              "      <td>0.172480</td>\n",
              "      <td>0.196110</td>\n",
              "      <td>0.202799</td>\n",
              "      <td>0.167736</td>\n",
              "      <td>0.034505</td>\n",
              "      <td>0.043099</td>\n",
              "      <td>-0.048147</td>\n",
              "      <td>0.355877</td>\n",
              "      <td>0.349107</td>\n",
              "      <td>0.355482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medianOfDiskByProcess</th>\n",
              "      <td>0.198296</td>\n",
              "      <td>0.352789</td>\n",
              "      <td>0.346805</td>\n",
              "      <td>0.354423</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999998</td>\n",
              "      <td>0.414388</td>\n",
              "      <td>0.414362</td>\n",
              "      <td>0.414457</td>\n",
              "      <td>0.163642</td>\n",
              "      <td>0.153216</td>\n",
              "      <td>0.148262</td>\n",
              "      <td>0.203691</td>\n",
              "      <td>0.200500</td>\n",
              "      <td>0.172787</td>\n",
              "      <td>0.196187</td>\n",
              "      <td>0.202869</td>\n",
              "      <td>0.167823</td>\n",
              "      <td>0.034566</td>\n",
              "      <td>0.043110</td>\n",
              "      <td>-0.048135</td>\n",
              "      <td>0.356079</td>\n",
              "      <td>0.349311</td>\n",
              "      <td>0.355662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maxDiskByProcess</th>\n",
              "      <td>0.198136</td>\n",
              "      <td>0.352981</td>\n",
              "      <td>0.347003</td>\n",
              "      <td>0.354647</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.999998</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.414295</td>\n",
              "      <td>0.414269</td>\n",
              "      <td>0.414364</td>\n",
              "      <td>0.163198</td>\n",
              "      <td>0.152771</td>\n",
              "      <td>0.147866</td>\n",
              "      <td>0.203564</td>\n",
              "      <td>0.200372</td>\n",
              "      <td>0.172651</td>\n",
              "      <td>0.196216</td>\n",
              "      <td>0.202911</td>\n",
              "      <td>0.167823</td>\n",
              "      <td>0.034566</td>\n",
              "      <td>0.043110</td>\n",
              "      <td>-0.048048</td>\n",
              "      <td>0.356274</td>\n",
              "      <td>0.349513</td>\n",
              "      <td>0.355892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>avgMemByProcess</th>\n",
              "      <td>0.194608</td>\n",
              "      <td>0.307152</td>\n",
              "      <td>0.314259</td>\n",
              "      <td>0.292993</td>\n",
              "      <td>0.414446</td>\n",
              "      <td>0.414388</td>\n",
              "      <td>0.414295</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999995</td>\n",
              "      <td>0.999982</td>\n",
              "      <td>0.397673</td>\n",
              "      <td>0.384650</td>\n",
              "      <td>0.404668</td>\n",
              "      <td>-0.106674</td>\n",
              "      <td>-0.111830</td>\n",
              "      <td>-0.117945</td>\n",
              "      <td>0.078945</td>\n",
              "      <td>0.067595</td>\n",
              "      <td>0.077842</td>\n",
              "      <td>0.054452</td>\n",
              "      <td>0.037025</td>\n",
              "      <td>0.048636</td>\n",
              "      <td>0.309798</td>\n",
              "      <td>0.315573</td>\n",
              "      <td>0.294544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medianOfMemByProcess</th>\n",
              "      <td>0.194459</td>\n",
              "      <td>0.307037</td>\n",
              "      <td>0.314168</td>\n",
              "      <td>0.292843</td>\n",
              "      <td>0.414420</td>\n",
              "      <td>0.414362</td>\n",
              "      <td>0.414269</td>\n",
              "      <td>0.999995</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999967</td>\n",
              "      <td>0.397742</td>\n",
              "      <td>0.384728</td>\n",
              "      <td>0.404681</td>\n",
              "      <td>-0.106717</td>\n",
              "      <td>-0.111871</td>\n",
              "      <td>-0.117991</td>\n",
              "      <td>0.078906</td>\n",
              "      <td>0.067577</td>\n",
              "      <td>0.077780</td>\n",
              "      <td>0.054318</td>\n",
              "      <td>0.036992</td>\n",
              "      <td>0.048518</td>\n",
              "      <td>0.309684</td>\n",
              "      <td>0.315485</td>\n",
              "      <td>0.294396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maxMemByProcess</th>\n",
              "      <td>0.194683</td>\n",
              "      <td>0.307333</td>\n",
              "      <td>0.314398</td>\n",
              "      <td>0.293270</td>\n",
              "      <td>0.414515</td>\n",
              "      <td>0.414457</td>\n",
              "      <td>0.414364</td>\n",
              "      <td>0.999982</td>\n",
              "      <td>0.999967</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.398054</td>\n",
              "      <td>0.385010</td>\n",
              "      <td>0.405087</td>\n",
              "      <td>-0.106671</td>\n",
              "      <td>-0.111822</td>\n",
              "      <td>-0.117903</td>\n",
              "      <td>0.078934</td>\n",
              "      <td>0.067569</td>\n",
              "      <td>0.077864</td>\n",
              "      <td>0.054734</td>\n",
              "      <td>0.037183</td>\n",
              "      <td>0.048759</td>\n",
              "      <td>0.309982</td>\n",
              "      <td>0.315711</td>\n",
              "      <td>0.294827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>percentageHeap</th>\n",
              "      <td>0.392303</td>\n",
              "      <td>0.304237</td>\n",
              "      <td>0.296399</td>\n",
              "      <td>0.295579</td>\n",
              "      <td>0.163432</td>\n",
              "      <td>0.163642</td>\n",
              "      <td>0.163198</td>\n",
              "      <td>0.397673</td>\n",
              "      <td>0.397742</td>\n",
              "      <td>0.398054</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.987015</td>\n",
              "      <td>0.934747</td>\n",
              "      <td>0.168068</td>\n",
              "      <td>0.166333</td>\n",
              "      <td>0.162646</td>\n",
              "      <td>0.146294</td>\n",
              "      <td>0.127659</td>\n",
              "      <td>0.138693</td>\n",
              "      <td>0.055691</td>\n",
              "      <td>0.005827</td>\n",
              "      <td>0.047670</td>\n",
              "      <td>0.303164</td>\n",
              "      <td>0.295219</td>\n",
              "      <td>0.295013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medianOfHeap</th>\n",
              "      <td>0.378232</td>\n",
              "      <td>0.298966</td>\n",
              "      <td>0.290886</td>\n",
              "      <td>0.292176</td>\n",
              "      <td>0.153003</td>\n",
              "      <td>0.153216</td>\n",
              "      <td>0.152771</td>\n",
              "      <td>0.384650</td>\n",
              "      <td>0.384728</td>\n",
              "      <td>0.385010</td>\n",
              "      <td>0.987015</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.912336</td>\n",
              "      <td>0.159448</td>\n",
              "      <td>0.157596</td>\n",
              "      <td>0.154535</td>\n",
              "      <td>0.141015</td>\n",
              "      <td>0.122810</td>\n",
              "      <td>0.134331</td>\n",
              "      <td>0.055486</td>\n",
              "      <td>0.007668</td>\n",
              "      <td>0.044015</td>\n",
              "      <td>0.297837</td>\n",
              "      <td>0.289473</td>\n",
              "      <td>0.291824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maxHeap</th>\n",
              "      <td>0.382723</td>\n",
              "      <td>0.430460</td>\n",
              "      <td>0.421043</td>\n",
              "      <td>0.421924</td>\n",
              "      <td>0.148056</td>\n",
              "      <td>0.148262</td>\n",
              "      <td>0.147866</td>\n",
              "      <td>0.404668</td>\n",
              "      <td>0.404681</td>\n",
              "      <td>0.405087</td>\n",
              "      <td>0.934747</td>\n",
              "      <td>0.912336</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.118295</td>\n",
              "      <td>0.116076</td>\n",
              "      <td>0.113204</td>\n",
              "      <td>0.120529</td>\n",
              "      <td>0.102364</td>\n",
              "      <td>0.117663</td>\n",
              "      <td>0.120934</td>\n",
              "      <td>0.008824</td>\n",
              "      <td>0.216640</td>\n",
              "      <td>0.430342</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.422232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>percentageWorkerThreads</th>\n",
              "      <td>0.363906</td>\n",
              "      <td>0.229928</td>\n",
              "      <td>0.202776</td>\n",
              "      <td>0.222130</td>\n",
              "      <td>0.203379</td>\n",
              "      <td>0.203691</td>\n",
              "      <td>0.203564</td>\n",
              "      <td>-0.106674</td>\n",
              "      <td>-0.106717</td>\n",
              "      <td>-0.106671</td>\n",
              "      <td>0.168068</td>\n",
              "      <td>0.159448</td>\n",
              "      <td>0.118295</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999581</td>\n",
              "      <td>0.994273</td>\n",
              "      <td>0.169632</td>\n",
              "      <td>0.154211</td>\n",
              "      <td>0.162158</td>\n",
              "      <td>0.091790</td>\n",
              "      <td>0.084602</td>\n",
              "      <td>-0.003810</td>\n",
              "      <td>0.234002</td>\n",
              "      <td>0.207867</td>\n",
              "      <td>0.225206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medianOfWorkerThreads</th>\n",
              "      <td>0.360850</td>\n",
              "      <td>0.224596</td>\n",
              "      <td>0.197404</td>\n",
              "      <td>0.216752</td>\n",
              "      <td>0.200188</td>\n",
              "      <td>0.200500</td>\n",
              "      <td>0.200372</td>\n",
              "      <td>-0.111830</td>\n",
              "      <td>-0.111871</td>\n",
              "      <td>-0.111822</td>\n",
              "      <td>0.166333</td>\n",
              "      <td>0.157596</td>\n",
              "      <td>0.116076</td>\n",
              "      <td>0.999581</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.994140</td>\n",
              "      <td>0.168397</td>\n",
              "      <td>0.153331</td>\n",
              "      <td>0.160777</td>\n",
              "      <td>0.089223</td>\n",
              "      <td>0.082583</td>\n",
              "      <td>-0.006271</td>\n",
              "      <td>0.228667</td>\n",
              "      <td>0.202476</td>\n",
              "      <td>0.219825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maxWorkerThreads</th>\n",
              "      <td>0.340872</td>\n",
              "      <td>0.192679</td>\n",
              "      <td>0.165443</td>\n",
              "      <td>0.186833</td>\n",
              "      <td>0.172480</td>\n",
              "      <td>0.172787</td>\n",
              "      <td>0.172651</td>\n",
              "      <td>-0.117945</td>\n",
              "      <td>-0.117991</td>\n",
              "      <td>-0.117903</td>\n",
              "      <td>0.162646</td>\n",
              "      <td>0.154535</td>\n",
              "      <td>0.113204</td>\n",
              "      <td>0.994273</td>\n",
              "      <td>0.994140</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.161256</td>\n",
              "      <td>0.146070</td>\n",
              "      <td>0.154653</td>\n",
              "      <td>0.078517</td>\n",
              "      <td>0.069190</td>\n",
              "      <td>-0.003580</td>\n",
              "      <td>0.196764</td>\n",
              "      <td>0.170544</td>\n",
              "      <td>0.189947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>avgInBufferSize</th>\n",
              "      <td>0.259144</td>\n",
              "      <td>0.110596</td>\n",
              "      <td>0.112358</td>\n",
              "      <td>0.119677</td>\n",
              "      <td>0.196110</td>\n",
              "      <td>0.196187</td>\n",
              "      <td>0.196216</td>\n",
              "      <td>0.078945</td>\n",
              "      <td>0.078906</td>\n",
              "      <td>0.078934</td>\n",
              "      <td>0.146294</td>\n",
              "      <td>0.141015</td>\n",
              "      <td>0.120529</td>\n",
              "      <td>0.169632</td>\n",
              "      <td>0.168397</td>\n",
              "      <td>0.161256</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.892437</td>\n",
              "      <td>0.905030</td>\n",
              "      <td>-0.048438</td>\n",
              "      <td>-0.036877</td>\n",
              "      <td>-0.081960</td>\n",
              "      <td>0.109641</td>\n",
              "      <td>0.111018</td>\n",
              "      <td>0.118630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medianOfInBuffer</th>\n",
              "      <td>0.224459</td>\n",
              "      <td>0.102793</td>\n",
              "      <td>0.099449</td>\n",
              "      <td>0.114637</td>\n",
              "      <td>0.202799</td>\n",
              "      <td>0.202869</td>\n",
              "      <td>0.202911</td>\n",
              "      <td>0.067595</td>\n",
              "      <td>0.067577</td>\n",
              "      <td>0.067569</td>\n",
              "      <td>0.127659</td>\n",
              "      <td>0.122810</td>\n",
              "      <td>0.102364</td>\n",
              "      <td>0.154211</td>\n",
              "      <td>0.153331</td>\n",
              "      <td>0.146070</td>\n",
              "      <td>0.892437</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.646894</td>\n",
              "      <td>-0.037471</td>\n",
              "      <td>-0.026530</td>\n",
              "      <td>-0.066854</td>\n",
              "      <td>0.102076</td>\n",
              "      <td>0.098453</td>\n",
              "      <td>0.113542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maxInBuffer</th>\n",
              "      <td>0.253975</td>\n",
              "      <td>0.107572</td>\n",
              "      <td>0.111570</td>\n",
              "      <td>0.118968</td>\n",
              "      <td>0.167736</td>\n",
              "      <td>0.167823</td>\n",
              "      <td>0.167823</td>\n",
              "      <td>0.077842</td>\n",
              "      <td>0.077780</td>\n",
              "      <td>0.077864</td>\n",
              "      <td>0.138693</td>\n",
              "      <td>0.134331</td>\n",
              "      <td>0.117663</td>\n",
              "      <td>0.162158</td>\n",
              "      <td>0.160777</td>\n",
              "      <td>0.154653</td>\n",
              "      <td>0.905030</td>\n",
              "      <td>0.646894</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.053806</td>\n",
              "      <td>-0.044547</td>\n",
              "      <td>-0.077118</td>\n",
              "      <td>0.106567</td>\n",
              "      <td>0.109987</td>\n",
              "      <td>0.118082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>percentageOutBuffer</th>\n",
              "      <td>-0.011664</td>\n",
              "      <td>0.340130</td>\n",
              "      <td>0.328117</td>\n",
              "      <td>0.318787</td>\n",
              "      <td>0.034505</td>\n",
              "      <td>0.034566</td>\n",
              "      <td>0.034566</td>\n",
              "      <td>0.054452</td>\n",
              "      <td>0.054318</td>\n",
              "      <td>0.054734</td>\n",
              "      <td>0.055691</td>\n",
              "      <td>0.055486</td>\n",
              "      <td>0.120934</td>\n",
              "      <td>0.091790</td>\n",
              "      <td>0.089223</td>\n",
              "      <td>0.078517</td>\n",
              "      <td>-0.048438</td>\n",
              "      <td>-0.037471</td>\n",
              "      <td>-0.053806</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.836206</td>\n",
              "      <td>0.741701</td>\n",
              "      <td>0.340238</td>\n",
              "      <td>0.328396</td>\n",
              "      <td>0.319939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medianOfOutBuffer</th>\n",
              "      <td>-0.037234</td>\n",
              "      <td>0.171820</td>\n",
              "      <td>0.165945</td>\n",
              "      <td>0.154946</td>\n",
              "      <td>0.043099</td>\n",
              "      <td>0.043110</td>\n",
              "      <td>0.043110</td>\n",
              "      <td>0.037025</td>\n",
              "      <td>0.036992</td>\n",
              "      <td>0.037183</td>\n",
              "      <td>0.005827</td>\n",
              "      <td>0.007668</td>\n",
              "      <td>0.008824</td>\n",
              "      <td>0.084602</td>\n",
              "      <td>0.082583</td>\n",
              "      <td>0.069190</td>\n",
              "      <td>-0.036877</td>\n",
              "      <td>-0.026530</td>\n",
              "      <td>-0.044547</td>\n",
              "      <td>0.836206</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.390885</td>\n",
              "      <td>0.171944</td>\n",
              "      <td>0.166706</td>\n",
              "      <td>0.156084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maxOutBuffer</th>\n",
              "      <td>-0.008066</td>\n",
              "      <td>0.449334</td>\n",
              "      <td>0.431969</td>\n",
              "      <td>0.433875</td>\n",
              "      <td>-0.048147</td>\n",
              "      <td>-0.048135</td>\n",
              "      <td>-0.048048</td>\n",
              "      <td>0.048636</td>\n",
              "      <td>0.048518</td>\n",
              "      <td>0.048759</td>\n",
              "      <td>0.047670</td>\n",
              "      <td>0.044015</td>\n",
              "      <td>0.216640</td>\n",
              "      <td>-0.003810</td>\n",
              "      <td>-0.006271</td>\n",
              "      <td>-0.003580</td>\n",
              "      <td>-0.081960</td>\n",
              "      <td>-0.066854</td>\n",
              "      <td>-0.077118</td>\n",
              "      <td>0.741701</td>\n",
              "      <td>0.390885</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.450604</td>\n",
              "      <td>0.433241</td>\n",
              "      <td>0.435706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>avgSystemCpuJmx</th>\n",
              "      <td>0.279643</td>\n",
              "      <td>0.997970</td>\n",
              "      <td>0.988494</td>\n",
              "      <td>0.981390</td>\n",
              "      <td>0.355877</td>\n",
              "      <td>0.356079</td>\n",
              "      <td>0.356274</td>\n",
              "      <td>0.309798</td>\n",
              "      <td>0.309684</td>\n",
              "      <td>0.309982</td>\n",
              "      <td>0.303164</td>\n",
              "      <td>0.297837</td>\n",
              "      <td>0.430342</td>\n",
              "      <td>0.234002</td>\n",
              "      <td>0.228667</td>\n",
              "      <td>0.196764</td>\n",
              "      <td>0.109641</td>\n",
              "      <td>0.102076</td>\n",
              "      <td>0.106567</td>\n",
              "      <td>0.340238</td>\n",
              "      <td>0.171944</td>\n",
              "      <td>0.450604</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.990586</td>\n",
              "      <td>0.982641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medianOfSystemCpu</th>\n",
              "      <td>0.259809</td>\n",
              "      <td>0.988611</td>\n",
              "      <td>0.998051</td>\n",
              "      <td>0.960049</td>\n",
              "      <td>0.349107</td>\n",
              "      <td>0.349311</td>\n",
              "      <td>0.349513</td>\n",
              "      <td>0.315573</td>\n",
              "      <td>0.315485</td>\n",
              "      <td>0.315711</td>\n",
              "      <td>0.295219</td>\n",
              "      <td>0.289473</td>\n",
              "      <td>0.420524</td>\n",
              "      <td>0.207867</td>\n",
              "      <td>0.202476</td>\n",
              "      <td>0.170544</td>\n",
              "      <td>0.111018</td>\n",
              "      <td>0.098453</td>\n",
              "      <td>0.109987</td>\n",
              "      <td>0.328396</td>\n",
              "      <td>0.166706</td>\n",
              "      <td>0.433241</td>\n",
              "      <td>0.990586</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.961253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maxSystemCpu</th>\n",
              "      <td>0.286002</td>\n",
              "      <td>0.980527</td>\n",
              "      <td>0.959106</td>\n",
              "      <td>0.998535</td>\n",
              "      <td>0.355482</td>\n",
              "      <td>0.355662</td>\n",
              "      <td>0.355892</td>\n",
              "      <td>0.294544</td>\n",
              "      <td>0.294396</td>\n",
              "      <td>0.294827</td>\n",
              "      <td>0.295013</td>\n",
              "      <td>0.291824</td>\n",
              "      <td>0.422232</td>\n",
              "      <td>0.225206</td>\n",
              "      <td>0.219825</td>\n",
              "      <td>0.189947</td>\n",
              "      <td>0.118630</td>\n",
              "      <td>0.113542</td>\n",
              "      <td>0.118082</td>\n",
              "      <td>0.319939</td>\n",
              "      <td>0.156084</td>\n",
              "      <td>0.435706</td>\n",
              "      <td>0.982641</td>\n",
              "      <td>0.961253</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         averageExecutionTime  ...  maxSystemCpu\n",
              "averageExecutionTime                 1.000000  ...      0.286002\n",
              "avgCpuByProcess                      0.280996  ...      0.980527\n",
              "medianOfCpuByProcess                 0.261092  ...      0.959106\n",
              "maxCpuByProcess                      0.287648  ...      0.998535\n",
              "avgDiskByProcess                     0.198137  ...      0.355482\n",
              "medianOfDiskByProcess                0.198296  ...      0.355662\n",
              "maxDiskByProcess                     0.198136  ...      0.355892\n",
              "avgMemByProcess                      0.194608  ...      0.294544\n",
              "medianOfMemByProcess                 0.194459  ...      0.294396\n",
              "maxMemByProcess                      0.194683  ...      0.294827\n",
              "percentageHeap                       0.392303  ...      0.295013\n",
              "medianOfHeap                         0.378232  ...      0.291824\n",
              "maxHeap                              0.382723  ...      0.422232\n",
              "percentageWorkerThreads              0.363906  ...      0.225206\n",
              "medianOfWorkerThreads                0.360850  ...      0.219825\n",
              "maxWorkerThreads                     0.340872  ...      0.189947\n",
              "avgInBufferSize                      0.259144  ...      0.118630\n",
              "medianOfInBuffer                     0.224459  ...      0.113542\n",
              "maxInBuffer                          0.253975  ...      0.118082\n",
              "percentageOutBuffer                 -0.011664  ...      0.319939\n",
              "medianOfOutBuffer                   -0.037234  ...      0.156084\n",
              "maxOutBuffer                        -0.008066  ...      0.435706\n",
              "avgSystemCpuJmx                      0.279643  ...      0.982641\n",
              "medianOfSystemCpu                    0.259809  ...      0.961253\n",
              "maxSystemCpu                         0.286002  ...      1.000000\n",
              "\n",
              "[25 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qU4P5QHm_AK"
      },
      "source": [
        "### Random Forest Classifier\r\n",
        "\r\n",
        "\r\n",
        "> Classes 3\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Features\r\n",
        "1.   avgCpuByProcess \r\n",
        "2.   maxDiskByProcess\r\n",
        "3.   maxHeap\r\n",
        "4.   percentageWorkerThreads\r\n",
        "\r\n",
        "\r\n",
        "> Model accuracy score =\r\n",
        " 0.6915422885572139\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3d7w-lY9pzdX",
        "outputId": "ac14b227-b96f-47a4-bbd7-47b2eb48cf68"
      },
      "source": [
        "#Random forest classifier\r\n",
        "#3 classes\r\n",
        "#4 features\r\n",
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxDiskByProcess','maxHeap','percentageWorkerThreads']]\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "model_rf = RandomForestClassifier(n_estimators=10,max_depth=4, random_state=0,min_samples_split=3)\r\n",
        "\r\n",
        "#class_weight='balanced_subsample'\r\n",
        "model_rf.fit(X_train,Y_train)#Fitting the model \r\n",
        "\r\n",
        "pred_rf=model_rf.predict(X_test)\r\n",
        "pred_rf_proba=model_rf.predict_proba(X_test)\r\n",
        "\r\n",
        "feat_importances = pd.Series(model_rf.feature_importances_, index=X_train.columns)\r\n",
        "feat_importances=feat_importances.sort_values()\r\n",
        "feat_importances.plot(kind='barh',figsize=(16,16))#Plotting feature importance\r\n",
        "\r\n",
        "print('Model Accuracy')\r\n",
        "print(model_rf.score(X_test,Y_test))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy\n",
            "0.6915422885572139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x1152 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WerfW4fKu4jN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyUic97Xodme"
      },
      "source": [
        "### Random Forest Classifier\r\n",
        "\r\n",
        "\r\n",
        "> Classes 4\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Features\r\n",
        "1.   avgCpuByProcess \r\n",
        "2.   maxDiskByProcess\r\n",
        "3.   maxHeap\r\n",
        "4.   percentageWorkerThreads\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "StpyqZQrsQfC",
        "outputId": "a3391d6b-8589-4831-c9c8-92d6badabb76"
      },
      "source": [
        "#Random forest classifier\r\n",
        "#4 classes\r\n",
        "#4 features\r\n",
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxDiskByProcess','maxHeap','percentageWorkerThreads']]\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "\r\n",
        "y=ras_metrics_training_LogReg['quintile']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "model_rf = RandomForestClassifier(n_estimators=10,max_depth=4, random_state=0,min_samples_split=3)\r\n",
        "\r\n",
        "#class_weight='balanced_subsample'\r\n",
        "model_rf.fit(X_train,Y_train)#Fitting the model \r\n",
        "\r\n",
        "pred_rf=model_rf.predict(X_test)\r\n",
        "pred_rf_proba=model_rf.predict_proba(X_test)\r\n",
        "\r\n",
        "feat_importances = pd.Series(model_rf.feature_importances_, index=X_train.columns)\r\n",
        "feat_importances=feat_importances.sort_values()\r\n",
        "feat_importances.plot(kind='barh',figsize=(16,16))#Plotting feature importance\r\n",
        "\r\n",
        "print('Model Accuracy')\r\n",
        "print(model_rf.score(X_test,Y_test))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy\n",
            "0.6218905472636815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x1152 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4y5v1FAolwo"
      },
      "source": [
        "### Random Forest Classifier\r\n",
        "\r\n",
        "\r\n",
        "> Classes 3\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Features\r\n",
        "1.   avgCpuByProcess \r\n",
        "2.   maxHeap\r\n",
        "3.   percentageWorkerThreads\r\n",
        "\r\n",
        "> Model accuracy score = 0.6865671641791045\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LTU7iRU_qDY3",
        "outputId": "026b300c-81ee-4e5e-a65a-db9ec5c3ee5f"
      },
      "source": [
        "#Random forest classifier\r\n",
        "#3 classes\r\n",
        "#3 features\r\n",
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxHeap','percentageWorkerThreads']]\r\n",
        "#ras_metrics_training_LogReg['buffer'] = ras_metrics['maxOutBuffer'] + ras_metrics['maxInBuffer'] \r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "model_rf = RandomForestClassifier(n_estimators=10,max_depth=4, random_state=0,min_samples_split=3)\r\n",
        "\r\n",
        "#class_weight='balanced_subsample'\r\n",
        "model_rf.fit(X_train,Y_train)#Fitting the model \r\n",
        "\r\n",
        "pred_rf=model_rf.predict(X_test)\r\n",
        "pred_rf_proba=model_rf.predict_proba(X_test)\r\n",
        "\r\n",
        "feat_importances = pd.Series(model_rf.feature_importances_, index=X_train.columns)\r\n",
        "feat_importances=feat_importances.sort_values()\r\n",
        "feat_importances.plot(kind='barh',figsize=(16,16))#Plotting feature importance\r\n",
        "\r\n",
        "print('Model Accuracy')\r\n",
        "print(model_rf.score(X_test,Y_test))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy\n",
            "0.6865671641791045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x1152 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od3-PrmopKru"
      },
      "source": [
        "### Random Forest Classifier\r\n",
        "\r\n",
        "\r\n",
        "> Classes 4\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Features\r\n",
        "1.   avgCpuByProcess \r\n",
        "2.   maxHeap\r\n",
        "3.   percentageWorkerThreads\r\n",
        "\r\n",
        "> Model accuracy score = 0.487"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PXmiRRYruCf7",
        "outputId": "e65750ac-ba7f-43f1-ecdf-7200b33c7cbf"
      },
      "source": [
        "#Random forest classifier\r\n",
        "#4 classes\r\n",
        "#3 features\r\n",
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxHeap','percentageWorkerThreads']]\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "\r\n",
        "y=ras_metrics_training_LogReg['quintile']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "model_rf = RandomForestClassifier(n_estimators=10,max_depth=4, random_state=0,min_samples_split=3)\r\n",
        "\r\n",
        "#class_weight='balanced_subsample'\r\n",
        "model_rf.fit(X_train,Y_train)#Fitting the model \r\n",
        "\r\n",
        "pred_rf=model_rf.predict(X_test)\r\n",
        "pred_rf_proba=model_rf.predict_proba(X_test)\r\n",
        "\r\n",
        "feat_importances = pd.Series(model_rf.feature_importances_, index=X_train.columns)\r\n",
        "feat_importances=feat_importances.sort_values()\r\n",
        "feat_importances.plot(kind='barh',figsize=(16,16))#Plotting feature importance\r\n",
        "\r\n",
        "print('Model Accuracy')\r\n",
        "print(model_rf.score(X_test,Y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy\n",
            "0.48756218905472637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x1152 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUVhhT15p2VS"
      },
      "source": [
        "### Random Forest Classifier\r\n",
        "\r\n",
        "\r\n",
        "> Classes 3\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Features\r\n",
        "1.   avgCpuByProcess \r\n",
        "2.   maxHeap\r\n",
        "3.   percentageWorkerThreads\r\n",
        "4.   maxDiskByProcess\r\n",
        "5.   Sum of inbuffer + outbuffer\r\n",
        "\r\n",
        "\r\n",
        "> Model accuracy score = 0.681592039800995"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CXBpPd5tu8Gi",
        "outputId": "111437cd-42dc-4ae1-cc4b-a42b1e0066e8"
      },
      "source": [
        "#Random forest classifier\r\n",
        "#3 classes\r\n",
        "#5 features\r\n",
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxDiskByProcess','maxHeap','percentageWorkerThreads']]\r\n",
        "ras_metrics_training_LogReg['buffer'] = ras_metrics['maxOutBuffer'] + ras_metrics['maxInBuffer'] \r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "ras_metrics_training_LogReg\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "model_rf = RandomForestClassifier(n_estimators=10,max_depth=4, random_state=0,min_samples_split=3)\r\n",
        "\r\n",
        "#class_weight='balanced_subsample'\r\n",
        "model_rf.fit(X_train,Y_train)#Fitting the model \r\n",
        "\r\n",
        "pred_rf=model_rf.predict(X_test)\r\n",
        "pred_rf_proba=model_rf.predict_proba(X_test)\r\n",
        "\r\n",
        "feat_importances = pd.Series(model_rf.feature_importances_, index=X_train.columns)\r\n",
        "feat_importances=feat_importances.sort_values()\r\n",
        "feat_importances.plot(kind='barh',figsize=(16,16))#Plotting feature importance\r\n",
        "\r\n",
        "print('Model Accuracy')\r\n",
        "print(model_rf.score(X_test,Y_test))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy\n",
            "0.681592039800995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x1152 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih5CpKreqn8p"
      },
      "source": [
        "## Random Forest Classifier summary\r\n",
        "Comparing all the models below is the conclusion\r\n",
        "\r\n",
        "\r\n",
        "1.   3 classes gave better results compared to 4 classes (69% vs 62%)\r\n",
        "2.   The best model is 3 class with 5 features having score of 0.69\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EJKS3QTIC9K"
      },
      "source": [
        "###Grid Search\r\n",
        "Performing grid search on hyper parameters to find the best Random Forest \r\n",
        "estimator\r\n",
        "\r\n",
        "Improve the score to 0.7114427860696517\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KqMUFNvHNzr",
        "outputId": "6bee58de-6996-43d6-c0ee-4cbeb4aa2a41"
      },
      "source": [
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxDiskByProcess','maxHeap','percentageWorkerThreads']]\r\n",
        "ras_metrics_training_LogReg['buffer'] = ras_metrics['maxOutBuffer'] + ras_metrics['maxInBuffer'] \r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "ras_metrics_training_LogReg\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "# Create the parameter grid based on the results of random search \r\n",
        "param_grid = {\r\n",
        "    'bootstrap': [True],\r\n",
        "    'max_depth': [80, 90, 100, 110],\r\n",
        "    'max_features': [3,5,7],\r\n",
        "    'min_samples_leaf': [3, 4, 5],\r\n",
        "    'min_samples_split': [3, 5, 7,10],\r\n",
        "    'n_estimators': [10, 30, 50]\r\n",
        "}\r\n",
        "# Create a based model\r\n",
        "rf = RandomForestClassifier(random_state=0)\r\n",
        "# Instantiate the grid search model\r\n",
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \r\n",
        "                          cv = 3, n_jobs = -1, verbose = 2)\r\n",
        "grid_search.fit(X_train,Y_train)\r\n",
        "print('Grid search score',grid_search.score(X_test,Y_test))\r\n",
        "Y_test_pred = grid_search.predict(X_test)\r\n",
        "print('The shapes are ', X_test.shape ,Y_test.shape, Y_test_pred.shape)\r\n",
        "from google.colab import files\r\n",
        "output = np.column_stack((X_test,Y_test,Y_test_pred))\r\n",
        "output_data = pd.DataFrame(output)#pd.merge(pd.DataFrame(X_test), pd.DataFrame(Y_test).merge(pd.DataFrame(Y_test_pred)))\r\n",
        "#output_data.head\r\n",
        "output_data.to_csv(r'output_random_forest_gs.csv',index=False, header=True)\r\n",
        "#files.download('output_random_forest_gs.csv')\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "conf_mx = confusion_matrix(Y_test, Y_test_pred)\r\n",
        "print('Confusion matrix', conf_mx)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    3.3s\n",
            "[Parallel(n_jobs=-1)]: Done 372 tasks      | elapsed:   18.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Grid search score 0.7114427860696517\n",
            "The shapes are  (201, 5) (201,) (201,)\n",
            "Confusion matrix [[50 11  0]\n",
            " [18 44  7]\n",
            " [ 4 18 49]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 1296 out of 1296 | elapsed:   58.2s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZBTM0NmrIRn"
      },
      "source": [
        "### Stochastic Gradient Descent Classifier\r\n",
        "> Classes 3\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Features\r\n",
        "1.   avgCpuByProcess \r\n",
        "2.   maxHeap\r\n",
        "3.   percentageWorkerThreads\r\n",
        "4.   Sum of inbuffer + outbuffer\r\n",
        "\r\n",
        "> Accuracy score =  0.5373134328358209"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IX4vuJjwomu",
        "outputId": "a20be14a-4998-4ae9-da49-c4ba9bde79c9"
      },
      "source": [
        "#SGD classifier\r\n",
        "#3 classes\r\n",
        "#4 features\r\n",
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxHeap','percentageWorkerThreads']]\r\n",
        "#ras_metrics_training_LogReg['buffer'] = ras_metrics['maxOutBuffer'] + ras_metrics['maxInBuffer'] \r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "ras_metrics_training_LogReg\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.linear_model import SGDClassifier\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "\r\n",
        "sgd_clf = SGDClassifier(random_state=42,max_iter=1000)\r\n",
        "sgd_clf.fit(X_train,Y_train)\r\n",
        "cross_val_score(sgd_clf, X_train, Y_train, cv=3, scoring=\"accuracy\")\r\n",
        "Y_pred = sgd_clf.predict(X_train)\r\n",
        "n_correct = sum(Y_pred == Y_train)\r\n",
        "print('Correct prediction #', n_correct/len(Y_train))\r\n",
        "print('Score', sgd_clf.score(X_test,Y_test))\r\n",
        "print ('Params', sgd_clf.get_params())\r\n",
        "print ('Decision function', sgd_clf.decision_function(X_train))\r\n",
        "#Y_pred"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct prediction # 0.57125\n",
            "Score 0.5621890547263682\n",
            "Params {'alpha': 0.0001, 'average': False, 'class_weight': None, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
            "Decision function [[ 1.99926696 -1.96318863 -2.93930125]\n",
            " [-0.35607239 -2.32458187 -0.71935236]\n",
            " [-1.04093142 -2.8718319   0.4282111 ]\n",
            " ...\n",
            " [ 0.78549623 -2.51443168 -1.2760328 ]\n",
            " [ 2.36380952 -2.15048766 -2.94955963]\n",
            " [-0.83785268 -2.23266272 -0.4338247 ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBUyERc1gP2C"
      },
      "source": [
        "### Stochastic Gradient Descent Classifier\r\n",
        "> Classes 3\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Features\r\n",
        "1.   avgCpuByProcess \r\n",
        "2.   maxHeap\r\n",
        "3.   percentageWorkerThreads\r\n",
        "4.   Sum of inbuffer + outbuffer\r\n",
        "5.   diskbyProcess\r\n",
        "\r\n",
        "> Accuracy score =  0.33"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXfTbcddzT3Q",
        "outputId": "52ed96fa-98ab-4e6a-98a3-572a5f538a16"
      },
      "source": [
        "#SGD classifier\r\n",
        "#3 classes\r\n",
        "#5 features\r\n",
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxHeap','percentageWorkerThreads','maxDiskByProcess']]\r\n",
        "ras_metrics_training_LogReg['buffer'] = ras_metrics['maxOutBuffer'] + ras_metrics['maxInBuffer'] \r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "ras_metrics_training_LogReg\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.linear_model import SGDClassifier\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "\r\n",
        "sgd_clf = SGDClassifier(random_state=42,max_iter=1000)\r\n",
        "sgd_clf.fit(X_train,Y_train)\r\n",
        "cross_val_score(sgd_clf, X_train, Y_train, cv=3, scoring=\"accuracy\")\r\n",
        "Y_pred = sgd_clf.predict(X_train)\r\n",
        "n_correct = sum(Y_pred == Y_train)\r\n",
        "print('Correct prediction #', n_correct/len(Y_train))\r\n",
        "print('Score', sgd_clf.score(X_test,Y_test))\r\n",
        "print ('Params', sgd_clf.get_params())\r\n",
        "print ('Decision function', sgd_clf.decision_function(X_train))\r\n",
        "Y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct prediction # 0.32875\n",
            "Score 0.35323383084577115\n",
            "Params {'alpha': 0.0001, 'average': False, 'class_weight': None, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
            "Decision function [[-7.45772720e+14 -5.43424273e+16  3.62875298e+16]\n",
            " [-1.02895997e+16 -7.49775111e+17  5.00667490e+17]\n",
            " [-1.93720884e+15 -1.41159133e+17  9.42599826e+16]\n",
            " ...\n",
            " [-1.02602003e+16 -7.47632854e+17  4.99236984e+17]\n",
            " [-7.45316842e+14 -5.43092087e+16  3.62653479e+16]\n",
            " [-1.93304374e+15 -1.40855634e+17  9.40573188e+16]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or000UllgalB"
      },
      "source": [
        "### Stochastic Gradient Descent Classifier\r\n",
        "> Classes 3\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Features\r\n",
        "1.   avgCpuByProcess \r\n",
        "2.   maxHeap\r\n",
        "3.   percentageWorkerThreads\r\n",
        "\r\n",
        "\r\n",
        "> Accuracy score =  0.56\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27_r_wJd0Jm0",
        "outputId": "88febbc9-52e4-4572-9e45-27cdb13c76d4"
      },
      "source": [
        "#SGD classifier\r\n",
        "#3 classes\r\n",
        "#3 features\r\n",
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxHeap']]\r\n",
        "#ras_metrics_training_LogReg['buffer'] = ras_metrics['maxOutBuffer'] + ras_metrics['maxInBuffer'] \r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "ras_metrics_training_LogReg\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.linear_model import SGDClassifier\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "\r\n",
        "sgd_clf = SGDClassifier(random_state=42,max_iter=1000)\r\n",
        "sgd_clf.fit(X_train,Y_train)\r\n",
        "cross_val_score(sgd_clf, X_train, Y_train, cv=3, scoring=\"accuracy\")\r\n",
        "Y_pred = sgd_clf.predict(X_train)\r\n",
        "Y_test_pred = sgd_clf.predict(X_test)\r\n",
        "n_correct = sum(Y_pred == Y_train)\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "conf_mx = confusion_matrix(Y_test, Y_test_pred)\r\n",
        "print(conf_mx)\r\n",
        "\r\n",
        "print('Correct prediction #', n_correct/len(Y_train))\r\n",
        "print('Score', sgd_clf.score(X_test,Y_test))\r\n",
        "print ('Params', sgd_clf.get_params())\r\n",
        "print ('Decision function', sgd_clf.decision_function(X_train))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[57  0  4]\n",
            " [47  3 19]\n",
            " [18  0 53]]\n",
            "Correct prediction # 0.55875\n",
            "Score 0.5621890547263682\n",
            "Params {'alpha': 0.0001, 'average': False, 'class_weight': None, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': None, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
            "Decision function [[ 2.29670366 -1.33161049 -2.58145535]\n",
            " [ 0.16217345 -1.72322509 -0.70681778]\n",
            " [-0.43577488 -2.18846358  0.27079609]\n",
            " ...\n",
            " [ 0.61502383 -1.81058034 -0.88762373]\n",
            " [ 2.31219354 -1.4572549  -2.43154194]\n",
            " [-0.39610338 -1.66911355 -0.41573006]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei0bOD3-gs_a"
      },
      "source": [
        "### Logistic Classifier\r\n",
        "> Classes 3\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Features\r\n",
        "1.   avgCpuByProcess \r\n",
        "2.   maxHeap\r\n",
        "3.   percentageWorkerThreads\r\n",
        "4.   Sum of inbuffer + outbuffer\r\n",
        "\r\n",
        "> Accuracy score =  0.5223880597014925\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL-uOPm359Mb",
        "outputId": "6b2c1ba0-4a4a-4acb-e7d6-35bab3d1e843"
      },
      "source": [
        "#Logistic classifier\r\n",
        "#3 classes\r\n",
        "#3 features\r\n",
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxHeap','percentageWorkerThreads']]\r\n",
        "#ras_metrics_training_LogReg['buffer'] = ras_metrics['maxOutBuffer'] + ras_metrics['maxInBuffer'] \r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "ras_metrics_training_LogReg\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "lr = LogisticRegression(random_state=0, solver='saga', multi_class='multinomial')\r\n",
        "lr.fit(X_train, Y_train)\r\n",
        "\r\n",
        "cross_val_score(lr, X_train, Y_train, cv=3, scoring=\"accuracy\")\r\n",
        "Y_pred = lr.predict(X_train)\r\n",
        "n_correct = sum(Y_pred == Y_train)\r\n",
        "print('Correct prediction #', n_correct/len(Y_train))\r\n",
        "print('Score', lr.score(X_test,Y_test))\r\n",
        "print ('Params', lr.get_params())\r\n",
        "#print ('Decision function', sgd_clf.decision_function(X_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct prediction # 0.5625\n",
            "Score 0.5223880597014925\n",
            "Params {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'multinomial', 'n_jobs': None, 'penalty': 'l2', 'random_state': 0, 'solver': 'saga', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USkDe1fzkaKN"
      },
      "source": [
        "### Logistic Regression CV\r\n",
        "> Classes 3\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Features\r\n",
        "1.   avgCpuByProcess \r\n",
        "2.   maxHeap\r\n",
        "3.   percentageWorkerThreads\r\n",
        "\r\n",
        "> Accuracy score =  0.61"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_umQXhTTsDwR",
        "outputId": "1fb06a76-0690-44ce-d128-513d8ccd9a6f"
      },
      "source": [
        "#sklearn.linear_model.LogisticRegressionCV();\r\n",
        "#Logistic classifier\r\n",
        "#3 classes\r\n",
        "#3 features\r\n",
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxHeap','percentageWorkerThreads']]\r\n",
        "#ras_metrics_training_LogReg['buffer'] = ras_metrics['maxOutBuffer'] + ras_metrics['maxInBuffer'] \r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "ras_metrics_training_LogReg\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.linear_model import LogisticRegressionCV\r\n",
        "lr = LogisticRegressionCV(random_state=0, solver='saga', multi_class='multinomial')\r\n",
        "lr.fit(X_train, Y_train)\r\n",
        "\r\n",
        "cross_val_score(lr, X_train, Y_train, cv=3, scoring=\"accuracy\")\r\n",
        "Y_pred = lr.predict(X_train)\r\n",
        "n_correct = sum(Y_pred == Y_train)\r\n",
        "print('Correct prediction #', n_correct/len(Y_train))\r\n",
        "print('Score', lr.score(X_test,Y_test))\r\n",
        "print ('Params', lr.get_params())\r\n",
        "#print ('Decision function', sgd_clf.decision_function(X_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Correct prediction # 0.61375\n",
            "Score 0.6119402985074627\n",
            "Params {'Cs': 10, 'class_weight': None, 'cv': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1.0, 'l1_ratios': None, 'max_iter': 100, 'multi_class': 'multinomial', 'n_jobs': None, 'penalty': 'l2', 'random_state': 0, 'refit': True, 'scoring': None, 'solver': 'saga', 'tol': 0.0001, 'verbose': 0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning:\n",
            "\n",
            "The max_iter was reached which means the coef_ did not converge\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKerrfFQiSDW"
      },
      "source": [
        "### Logistic Regression CV lgbfs solver\r\n",
        "> Classes 3\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Features\r\n",
        "1.   avgCpuByProcess \r\n",
        "2.   maxHeap\r\n",
        "3.   percentageWorkerThreads\r\n",
        "\r\n",
        "> Accuracy score =  0.50\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uk1L-gi65kR",
        "outputId": "108f887d-661d-4ca8-8fab-2848ff161b1b"
      },
      "source": [
        "#sklearn.linear_model.LogisticRegressionCV();\r\n",
        "#Logistic classifier\r\n",
        "#3 classes\r\n",
        "#2 features\r\n",
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','percentageWorkerThreads','maxHeap']]\r\n",
        "#ras_metrics_training_LogReg['buffer'] = ras_metrics['maxOutBuffer'] + ras_metrics['maxInBuffer'] \r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "ras_metrics_training_LogReg\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.linear_model import LogisticRegressionCV\r\n",
        "lr = LogisticRegressionCV(random_state=0, solver='lbfgs', multi_class='multinomial')\r\n",
        "lr.fit(X_train, Y_train)\r\n",
        "\r\n",
        "cross_val_score(lr, X_train, Y_train, cv=3, scoring=\"accuracy\")\r\n",
        "Y_pred = lr.predict(X_train)\r\n",
        "n_correct = sum(Y_pred == Y_train)\r\n",
        "print('Correct prediction #', n_correct/len(Y_train))\r\n",
        "print('Score', lr.score(X_test,Y_test))\r\n",
        "print ('Params', lr.get_params())\r\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Correct prediction # 0.55375\n",
            "Score 0.5024875621890548\n",
            "Params {'Cs': 10, 'class_weight': None, 'cv': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1.0, 'l1_ratios': None, 'max_iter': 100, 'multi_class': 'multinomial', 'n_jobs': None, 'penalty': 'l2', 'random_state': 0, 'refit': True, 'scoring': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpFzKRuG7rUg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyA9GaMdimsX"
      },
      "source": [
        "### OneVsRestClassifier with Non linear (polynomial) classification\r\n",
        "> Classes 3\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Features\r\n",
        "1.   avgCpuByProcess \r\n",
        "2.   maxHeap\r\n",
        "3.   percentageWorkerThreads\r\n",
        "\r\n",
        "> Accuracy score =  0.57"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "WxCB0DaUvuz8",
        "outputId": "a1f47b36-9e7a-442d-e45d-7d5657794a45"
      },
      "source": [
        "#sklearn.linear_model.OneVsRestClassifier();\r\n",
        "#OneVsRest classifier\r\n",
        "#3 classes\r\n",
        "#3 features\r\n",
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxHeap','percentageWorkerThreads']]\r\n",
        "#ras_metrics_training_LogReg['buffer'] = ras_metrics['maxOutBuffer'] + ras_metrics['maxInBuffer'] \r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "ras_metrics_training_LogReg\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.multiclass import OneVsRestClassifier\r\n",
        "from sklearn.svm import SVC\r\n",
        "\r\n",
        "lr = OneVsRestClassifier(SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\r\n",
        "lr.fit(X_train, Y_train)\r\n",
        "\r\n",
        "#cross_val_score(lr, X_train, Y_train, cv=3, scoring=\"accuracy\")\r\n",
        "Y_pred = lr.predict(X_train)\r\n",
        "n_correct = sum(Y_pred == Y_train)\r\n",
        "print('Correct prediction #', n_correct/len(Y_train))\r\n",
        "print('Score', lr.score(X_test,Y_test))\r\n",
        "print ('Params', lr.get_params())\r\n",
        "from sklearn.model_selection import cross_val_predict\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "Y_test_pred = cross_val_predict(poly_kernel_svm_clf, X_test, Y_test, cv=3)\r\n",
        "Y_test_pred = poly_kernel_svm_clf.predict(X_test)\r\n",
        "conf_mx = confusion_matrix(Y_test, Y_test_pred)\r\n",
        "conf_mx\r\n",
        "print('The shapes are ', X_test.shape ,Y_test.shape, Y_test_pred.shape)\r\n",
        "from google.colab import files\r\n",
        "output = np.column_stack((X_test,Y_test,Y_test_pred))\r\n",
        "output_data = pd.DataFrame(output)#pd.merge(pd.DataFrame(X_test), pd.DataFrame(Y_test).merge(pd.DataFrame(Y_test_pred)))\r\n",
        "#output_data.head\r\n",
        "output_data.to_csv(r'output2.csv',index=False, header=True)\r\n",
        "files.download('output2.csv')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct prediction # 0.60125\n",
            "Score 0.5771144278606966\n",
            "Params {'estimator__C': 5, 'estimator__break_ties': False, 'estimator__cache_size': 200, 'estimator__class_weight': None, 'estimator__coef0': 1, 'estimator__decision_function_shape': 'ovr', 'estimator__degree': 3, 'estimator__gamma': 'scale', 'estimator__kernel': 'poly', 'estimator__max_iter': -1, 'estimator__probability': False, 'estimator__random_state': None, 'estimator__shrinking': True, 'estimator__tol': 0.001, 'estimator__verbose': False, 'estimator': SVC(C=5, break_ties=False, cache_size=200, class_weight=None, coef0=1,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='poly',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False), 'n_jobs': None}\n",
            "The shapes are  (201, 3) (201,) (201,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_06706249-d6cf-4fa0-b94c-6613f599a23e\", \"output2.csv\", 7418)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dZYKWU1i_4C"
      },
      "source": [
        "### Support Vector machine\r\n",
        "> Classes 3\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Features\r\n",
        "1.   avgCpuByProcess \r\n",
        "2.   maxHeap\r\n",
        "3.   percentageWorkerThreads\r\n",
        "\r\n",
        "> Accuracy score =  0.67\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkOY1xAq17QO",
        "outputId": "9fd2af23-e735-4c5e-c20c-3f35318d9f67"
      },
      "source": [
        "#sklearn.linear_model.SVC();\r\n",
        "\r\n",
        "#3 classes\r\n",
        "#3 features\r\n",
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','maxHeap','percentageWorkerThreads']]\r\n",
        "#ras_metrics_training_LogReg['buffer'] = ras_metrics['maxOutBuffer'] + ras_metrics['maxInBuffer'] \r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "ras_metrics_training_LogReg\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.multiclass import OneVsRestClassifier\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.svm import SVC\r\n",
        "poly_kernel_svm_clf = Pipeline([\r\n",
        "        (\"scaler\", StandardScaler()),\r\n",
        "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=1))\r\n",
        "    ])\r\n",
        "poly_kernel_svm_clf.fit(X_train, Y_train)\r\n",
        "print('Test Score', poly_kernel_svm_clf.score(X_test,Y_test))\r\n",
        "from sklearn.model_selection import cross_val_predict\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "Y_train_pred = cross_val_predict(poly_kernel_svm_clf, X_test, Y_test, cv=3)\r\n",
        "conf_mx = confusion_matrix(Y_test, Y_train_pred)\r\n",
        "conf_mx"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Score 0.5373134328358209\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[20, 34,  7],\n",
              "       [27, 25, 17],\n",
              "       [ 7, 26, 38]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4FlUqKaQrL8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO_9jp0c4d-F"
      },
      "source": [
        "### XGBoost Classifier\r\n",
        "> Classes 3\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Features\r\n",
        "1.   avgCpuByProcess \r\n",
        "2.   maxHeap\r\n",
        "3.   percentageWorkerThreads\r\n",
        "4.   maxDiskByProcess\r\n",
        "\r\n",
        "> Accuracy score =  0.711"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQb7RghR7u_B",
        "outputId": "7f8ba601-c3ec-4501-fe09-cb4a3cd03be2"
      },
      "source": [
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxDiskByProcess','maxHeap','percentageWorkerThreads']]\r\n",
        "#ras_metrics_training_LogReg['buffer'] = ras_metrics['maxOutBuffer'] + ras_metrics['maxInBuffer'] \r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "ras_metrics_training_LogReg\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.linear_model import LogisticRegressionCV\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "xgBoost = XGBClassifier()\r\n",
        "xgBoost.fit(X_train, Y_train)\r\n",
        "\r\n",
        "#cross_val_score(lr, X_train, Y_train, cv=3, scoring=\"accuracy\")\r\n",
        "Y_pred = xgBoost.predict(X_train)\r\n",
        "Y_pred_test = xgBoost.predict(X_test)\r\n",
        "n_correct = sum(Y_pred == Y_train)\r\n",
        "print('Feature importance', xgBoost.feature_importances_)\r\n",
        "print('Score', xgBoost.score(X_test,Y_test))\r\n",
        "print('Accuracy score train data#', accuracy_score(Y_pred,Y_train))\r\n",
        "print('Accuracy score test data#', accuracy_score(Y_pred_test,Y_test))\r\n",
        "#print('Score', lr.score(X_test,Y_test))\r\n",
        "#print ('Params', lr.get_params())\r\n",
        "from sklearn.model_selection import cross_val_predict\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "Y_train_pred = cross_val_predict(poly_kernel_svm_clf, X_test, Y_test, cv=3)\r\n",
        "conf_mx = confusion_matrix(Y_test, Y_train_pred)\r\n",
        "conf_mx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Feature importance [0.4368499  0.15315212 0.20692913 0.20306878]\n",
            "Score 0.7114427860696517\n",
            "Accuracy score train data# 0.82125\n",
            "Accuracy score test data# 0.7114427860696517\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[30, 25,  6],\n",
              "       [17, 28, 24],\n",
              "       [ 2, 21, 48]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIanI-X5St0x",
        "outputId": "a19945ac-9222-4f89-a5f8-b17ec9e809b1"
      },
      "source": [
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxHeap','percentageWorkerThreads']]\r\n",
        "#ras_metrics_training_LogReg['buffer'] = ras_metrics['maxOutBuffer'] + ras_metrics['maxInBuffer'] \r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "ras_metrics_training_LogReg\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "from sklearn.linear_model import LogisticRegressionCV\r\n",
        "\r\n",
        "pipeline = Pipeline([\r\n",
        "    (\"kmeans\", KMeans(n_clusters=100)),\r\n",
        "    (\"log_reg\", LogisticRegressionCV()),\r\n",
        "])\r\n",
        "pipeline.fit(X_train, Y_train)\r\n",
        "pipeline.score(X_test,Y_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.43781094527363185"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqdKQwfgcZHj"
      },
      "source": [
        "Building an Neural Network based classifier\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08y6se6mclBb"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2OL_IF8McsYt",
        "outputId": "7d8cb404-719b-4ac9-bb1d-180b0dd6067b"
      },
      "source": [
        "tf.__version__\r\n",
        "keras.__version__\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Dc2H5D0daU6",
        "outputId": "e0437bf0-1302-4992-dfb4-1960bd347454"
      },
      "source": [
        "\r\n",
        "ras_metrics_training_LogReg =ras_metrics[['averageExecutionTime','avgCpuByProcess','maxDiskByProcess','maxHeap','percentageWorkerThreads']]\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 4, labels=False)\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "#ras_metrics_training_LogReg\r\n",
        "ras_metrics_training_LogReg['quintile_3']=pd.qcut(ras_metrics_training_LogReg['averageExecutionTime'], 3, labels=False)\r\n",
        "\r\n",
        "\r\n",
        "y=ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "del ras_metrics_training_LogReg['averageExecutionTime']\r\n",
        "del ras_metrics_training_LogReg['quintile']\r\n",
        "del ras_metrics_training_LogReg['quintile_3']\r\n",
        "\r\n",
        "X=ras_metrics_training_LogReg.iloc[:,:-1]\r\n",
        "X=ras_metrics_training_LogReg\r\n",
        "X_train_full,X_test,Y_train_full,Y_test=train_test_split(X, y,test_size=0.2,random_state=1)\r\n",
        "X_train,X_valid,Y_train,Y_valid=train_test_split(X_train_full, Y_train_full,test_size=0.25,random_state=1)\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "scaler = StandardScaler()\r\n",
        "X_train = scaler.fit_transform(X_train)\r\n",
        "X_valid = scaler.transform(X_valid)\r\n",
        "X_test = scaler.transform(X_test)\r\n",
        "\r\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C9EJjBsc-1b"
      },
      "source": [
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Dense(300, activation=\"relu\",input_shape=X_train.shape[1:]),\r\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\r\n",
        "    keras.layers.Dense(3, activation=\"softmax\")\r\n",
        "])\r\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\r\n",
        "              optimizer=\"sgd\",\r\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et1WoxI8dHsw",
        "outputId": "0331e1a1-06a0-4630-b0d2-c8bd75e19686"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 300)               1500      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 303       \n",
            "=================================================================\n",
            "Total params: 31,903\n",
            "Trainable params: 31,903\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_ddZqomhMNO",
        "outputId": "db1c98b9-c739-4754-84bd-67dab3508b9c"
      },
      "source": [
        "history = model.fit(X_train, Y_train, epochs=30,\r\n",
        "                    validation_data=(X_valid, Y_valid))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "19/19 [==============================] - 0s 13ms/step - loss: 1.1366 - accuracy: 0.1821 - val_loss: 1.1039 - val_accuracy: 0.3250\n",
            "Epoch 2/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0950 - accuracy: 0.3860 - val_loss: 1.0677 - val_accuracy: 0.4750\n",
            "Epoch 3/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0487 - accuracy: 0.5081 - val_loss: 1.0372 - val_accuracy: 0.5300\n",
            "Epoch 4/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0151 - accuracy: 0.5487 - val_loss: 1.0099 - val_accuracy: 0.5500\n",
            "Epoch 5/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9997 - accuracy: 0.5164 - val_loss: 0.9851 - val_accuracy: 0.5700\n",
            "Epoch 6/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9712 - accuracy: 0.5469 - val_loss: 0.9618 - val_accuracy: 0.5800\n",
            "Epoch 7/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9300 - accuracy: 0.5980 - val_loss: 0.9404 - val_accuracy: 0.5900\n",
            "Epoch 8/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9365 - accuracy: 0.5408 - val_loss: 0.9209 - val_accuracy: 0.5900\n",
            "Epoch 9/30\n",
            "19/19 [==============================] - 0s 13ms/step - loss: 0.8790 - accuracy: 0.6068 - val_loss: 0.9030 - val_accuracy: 0.5950\n",
            "Epoch 10/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.8780 - accuracy: 0.5879 - val_loss: 0.8870 - val_accuracy: 0.5850\n",
            "Epoch 11/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.8779 - accuracy: 0.5847 - val_loss: 0.8727 - val_accuracy: 0.5900\n",
            "Epoch 12/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.8549 - accuracy: 0.5831 - val_loss: 0.8602 - val_accuracy: 0.6050\n",
            "Epoch 13/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.8330 - accuracy: 0.6050 - val_loss: 0.8490 - val_accuracy: 0.6200\n",
            "Epoch 14/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.8423 - accuracy: 0.5659 - val_loss: 0.8389 - val_accuracy: 0.6150\n",
            "Epoch 15/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.8261 - accuracy: 0.5799 - val_loss: 0.8299 - val_accuracy: 0.6250\n",
            "Epoch 16/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.8014 - accuracy: 0.6195 - val_loss: 0.8220 - val_accuracy: 0.6350\n",
            "Epoch 17/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.8030 - accuracy: 0.5875 - val_loss: 0.8147 - val_accuracy: 0.6300\n",
            "Epoch 18/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.7860 - accuracy: 0.6162 - val_loss: 0.8081 - val_accuracy: 0.6300\n",
            "Epoch 19/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.7411 - accuracy: 0.6533 - val_loss: 0.8021 - val_accuracy: 0.6250\n",
            "Epoch 20/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.7805 - accuracy: 0.6255 - val_loss: 0.7965 - val_accuracy: 0.6250\n",
            "Epoch 21/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.7470 - accuracy: 0.6502 - val_loss: 0.7917 - val_accuracy: 0.6250\n",
            "Epoch 22/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.7655 - accuracy: 0.6514 - val_loss: 0.7871 - val_accuracy: 0.6200\n",
            "Epoch 23/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.7473 - accuracy: 0.6695 - val_loss: 0.7826 - val_accuracy: 0.6200\n",
            "Epoch 24/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.7280 - accuracy: 0.6358 - val_loss: 0.7787 - val_accuracy: 0.6100\n",
            "Epoch 25/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.7672 - accuracy: 0.6276 - val_loss: 0.7751 - val_accuracy: 0.6100\n",
            "Epoch 26/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.7655 - accuracy: 0.6145 - val_loss: 0.7718 - val_accuracy: 0.6100\n",
            "Epoch 27/30\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.7602 - accuracy: 0.6190 - val_loss: 0.7688 - val_accuracy: 0.6050\n",
            "Epoch 28/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.7520 - accuracy: 0.6440 - val_loss: 0.7659 - val_accuracy: 0.6050\n",
            "Epoch 29/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.7500 - accuracy: 0.6388 - val_loss: 0.7632 - val_accuracy: 0.6100\n",
            "Epoch 30/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.7284 - accuracy: 0.6524 - val_loss: 0.7607 - val_accuracy: 0.6150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "NqwzGrAahl5h",
        "outputId": "e10d0758-d9c0-4160-f717-bca08bf14055"
      },
      "source": [
        "import pandas as pd\r\n",
        "import os\r\n",
        "\r\n",
        "CHAPTER_ID = \"ann\"\r\n",
        "PROJECT_ROOT_DIR = \".\"\r\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\r\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\r\n",
        "\r\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\r\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\r\n",
        "    print(\"Saving figure\", fig_id)\r\n",
        "    if tight_layout:\r\n",
        "        plt.tight_layout()\r\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\r\n",
        "\r\n",
        "# Ignore useless warnings (see SciPy issue #5998)\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\r\n",
        "\r\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\r\n",
        "plt.grid(True)\r\n",
        "plt.gca().set_ylim(0, 1)\r\n",
        "#save_fig(\"keras_learning_curves_plot\")\r\n",
        "plt.show()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL9J8AaBihFx",
        "outputId": "67dbdcf9-9aa0-45fd-b7b1-440d0fd1e478"
      },
      "source": [
        "\r\n",
        "\r\n",
        "model.evaluate(X_test, Y_test)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 0s 2ms/step - loss: 0.7659 - accuracy: 0.6716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7659160494804382, 0.6716417670249939]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA4v1U1Ou7wa"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5arz4ajOsVAV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFyJZTJXkj54"
      },
      "source": [
        "X_test\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vz3yaOXrN0d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "2NQ72kBEjJyb",
        "outputId": "6df5c852-564c-4248-a15b-6a8883185fa9"
      },
      "source": [
        "from google.colab import files\r\n",
        "Y_test_pred =model.predict_classes(X_test)\r\n",
        "Y_test_pred\r\n",
        "output = np.column_stack((X_test,Y_test,Y_test_pred))\r\n",
        "output_data = pd.DataFrame(output)#pd.merge(pd.DataFrame(X_test), pd.DataFrame(Y_test).merge(pd.DataFrame(Y_test_pred)))\r\n",
        "#output_data.head\r\n",
        "output_data.to_csv(r'output_neural_network.csv',index=False, header=True)\r\n",
        "files.download('output_neural_network.csv')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning:\n",
            "\n",
            "`model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8fa11c21-1dcb-4e4e-b855-24a17b614c13\", \"output_neural_network.csv\", 17409)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AEGjbVfuymx"
      },
      "source": [
        "model.con"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzrciQ7ZrQJJ",
        "outputId": "7d5a4997-7e2b-4cb4-8d4a-0eb7b3a570df"
      },
      "source": [
        "keras.backend.clear_session()\r\n",
        "np.random.seed(42)\r\n",
        "tf.random.set_seed(42)\r\n",
        "\r\n",
        "def build_model( input_shape=X_train.shape[1:],n_hidden=3, n_neurons=100, learning_rate=3e-3):\r\n",
        "    model = keras.models.Sequential()\r\n",
        "    model.add(keras.layers.Dense(300,activation=\"relu\",input_shape=input_shape))\r\n",
        "    for layer in range(n_hidden):\r\n",
        "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\r\n",
        "    model.add( keras.layers.Dense(3, activation=\"softmax\"))\r\n",
        "    optimizer = keras.optimizers.SGD(lr=learning_rate)\r\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\r\n",
        "    return model\r\n",
        "\r\n",
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\r\n",
        "\r\n",
        "#\r\n",
        "\r\n",
        "keras_reg.fit(X_train, Y_train, epochs=30,\r\n",
        "              validation_data=(X_valid, Y_valid),\r\n",
        "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "19/19 [==============================] - 1s 13ms/step - loss: 1.1027 - accuracy: 0.3765 - val_loss: 1.0991 - val_accuracy: 0.3450\n",
            "Epoch 2/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0967 - accuracy: 0.3477 - val_loss: 1.0926 - val_accuracy: 0.3600\n",
            "Epoch 3/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0922 - accuracy: 0.3512 - val_loss: 1.0863 - val_accuracy: 0.3800\n",
            "Epoch 4/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0816 - accuracy: 0.4056 - val_loss: 1.0803 - val_accuracy: 0.3850\n",
            "Epoch 5/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0759 - accuracy: 0.4356 - val_loss: 1.0746 - val_accuracy: 0.4600\n",
            "Epoch 6/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0705 - accuracy: 0.5153 - val_loss: 1.0690 - val_accuracy: 0.5050\n",
            "Epoch 7/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0659 - accuracy: 0.5421 - val_loss: 1.0636 - val_accuracy: 0.5450\n",
            "Epoch 8/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0563 - accuracy: 0.5946 - val_loss: 1.0582 - val_accuracy: 0.5700\n",
            "Epoch 9/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0493 - accuracy: 0.5953 - val_loss: 1.0530 - val_accuracy: 0.5750\n",
            "Epoch 10/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0505 - accuracy: 0.5788 - val_loss: 1.0477 - val_accuracy: 0.5850\n",
            "Epoch 11/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0403 - accuracy: 0.6031 - val_loss: 1.0425 - val_accuracy: 0.5900\n",
            "Epoch 12/30\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 1.0341 - accuracy: 0.5973 - val_loss: 1.0372 - val_accuracy: 0.6000\n",
            "Epoch 13/30\n",
            "19/19 [==============================] - 0s 15ms/step - loss: 1.0281 - accuracy: 0.6113 - val_loss: 1.0320 - val_accuracy: 0.5950\n",
            "Epoch 14/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0264 - accuracy: 0.5909 - val_loss: 1.0266 - val_accuracy: 0.5900\n",
            "Epoch 15/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0172 - accuracy: 0.5999 - val_loss: 1.0212 - val_accuracy: 0.5800\n",
            "Epoch 16/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0119 - accuracy: 0.5929 - val_loss: 1.0158 - val_accuracy: 0.5750\n",
            "Epoch 17/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0110 - accuracy: 0.5766 - val_loss: 1.0102 - val_accuracy: 0.5700\n",
            "Epoch 18/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 1.0109 - accuracy: 0.5567 - val_loss: 1.0046 - val_accuracy: 0.5700\n",
            "Epoch 19/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9860 - accuracy: 0.6134 - val_loss: 0.9989 - val_accuracy: 0.5750\n",
            "Epoch 20/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9883 - accuracy: 0.5809 - val_loss: 0.9931 - val_accuracy: 0.5750\n",
            "Epoch 21/30\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 0.9787 - accuracy: 0.5884 - val_loss: 0.9871 - val_accuracy: 0.5800\n",
            "Epoch 22/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9811 - accuracy: 0.5772 - val_loss: 0.9811 - val_accuracy: 0.5800\n",
            "Epoch 23/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9673 - accuracy: 0.5816 - val_loss: 0.9750 - val_accuracy: 0.5800\n",
            "Epoch 24/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9623 - accuracy: 0.5655 - val_loss: 0.9689 - val_accuracy: 0.5800\n",
            "Epoch 25/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9547 - accuracy: 0.5853 - val_loss: 0.9629 - val_accuracy: 0.5800\n",
            "Epoch 26/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9522 - accuracy: 0.5948 - val_loss: 0.9569 - val_accuracy: 0.5800\n",
            "Epoch 27/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9452 - accuracy: 0.5787 - val_loss: 0.9509 - val_accuracy: 0.5800\n",
            "Epoch 28/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9517 - accuracy: 0.5655 - val_loss: 0.9448 - val_accuracy: 0.5800\n",
            "Epoch 29/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9272 - accuracy: 0.5782 - val_loss: 0.9389 - val_accuracy: 0.5850\n",
            "Epoch 30/30\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.9280 - accuracy: 0.5766 - val_loss: 0.9330 - val_accuracy: 0.5850\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3711ea50f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lOilLmusWIp",
        "outputId": "89a1e7af-b13d-4969-9429-1d16df9de66d"
      },
      "source": [
        "keras_reg.score(X_test, Y_test)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 0s 2ms/step - loss: 0.9137 - accuracy: 0.5771\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.9136760830879211"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    }
  ]
}